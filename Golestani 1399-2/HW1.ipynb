{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" class=\"alert alert-block alert-info\"> <font size=\"5\" face=\"HM XNiloofar\"> \n",
    "<b> یادگیری تقویتی - دکتر گلستانی: تمرین شبیه‌سازی اول </b>\n",
    "</font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "<br />\n",
    "مهلت تحویل تمرین: ۶ خرداد ۱۴۰۱ <br />\n",
    "فقط قسمت‌های <code>TODO</code> را پر کنید و  Jupyter Notebook تکمیل شده را به فرمت <code>ipynb</code> و <code>html</code> به عنوان گزارش خود در CW آپلود کنید. <br />\n",
    "ابهامات و سوالات خود در مورد این تمرین را می توانید با طراح تمرین مطرح کنید. <br />\n",
    "<div dir=\"ltr\">@hamidreza_ehteram</div> <br />  \n",
    "در این تمرین شما در یک محیط بازی تعریف شده الگوریتم‌های Policy Iteration   و Value Iteration را به عنوان دو الگوریتم model-based و Q-Learning را برای model-free  پیاده‌سازی می‌کنید و با تاثیر پارامتر‌های مختلف در الگوریتم‌ها آشنا می‌شوید. <br />\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***You can jump directly to these sections:***\n",
    "\n",
    "+ [A. An experience with the environment](#A.-An-experience-with-the-environment)\n",
    "+ [B. Policy Iteration (model-based)](#B.-Policy-Iteration)\n",
    "+ [C. Value Iteration (model-based)](#C.-Value-Iteration)\n",
    "+ [D. Q-Learning (model-free)](#D.-Q-Learning)\n",
    "+ [Give Us Feedback](#Give-Us-Feedback)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> <b>  آخرین گنج قارون: </b>\n",
    " یک تیم کاوشگری در یکی از مناطق باستانی، نقشه‌ی گنجی که در زیرزمینی متروکه مخفی شده است را پیدا می‌کند. در نقشه محل گنج 💰، کلید آن 🔑 و گودال‌های مرگ 🕳️ نشان داده شده  است. از آنجا که دسترسی به این گنج فقط به کمک یک ربات کاوشگر 🤖 ممکن است، این تیم از شما به عنوان یک متخصص علوم داده می‌خواهد تا یک ربات  (Agent) را برای این هدف آموزش دهید.\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin: 20px\" class=\"alert alert-block alert-success\"> <font size=\"5\">\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> <b>  محیط پیش‌فرض بازی: </b>  </font> </div>     \n",
    "<b> <center> <tt>\n",
    "--------------------------------<br />\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;-&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|<br />\n",
    "--------------------------------<br />\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;#&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;#&ensp;&ensp;|<br />\n",
    "--------------------------------<br />\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;$&ensp;&ensp;|<br />\n",
    "--------------------------------<br />\n",
    "&ensp;&ensp;&ensp;A&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|<br />\n",
    "</tt> </center> </b> </font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">  \n",
    "در این محیط ربات با <code>A</code>، گنج با <code>$</code>، کلید با <code>-</code> و گودال‌های مرگ با <code>#</code> نشان داده شده است. بازی به این صورت است که در ابتدا ربات به‌ صورت تصادفی در یکی از خانه‌های بازی (به غیر از خانه‌ی گنج، کلید و گودال‌های مرگ) قرار می‌گیرد. در هر مرحله ربات می‌تواند یکی از Action‌ های راست، بالا، چپ و پایین را انجام دهید. از آنجا که این محیط لغزنده است، ربات به احتمال <code>p</code> در جهت درست و به احتمال <code>q = 1-p</code> به اشتباه در جهت ۹۰ درجه مثلثاتی اکشن انتخاب شده یک گام حرکت می‌کند. اگر ربات به دیوار بازی برخورد کند در همان موقعیت قبلی خود می‌ماند و اگر وارد گودال‌های مرگ شود بازی تمامی می‌شود و برای همیشه در آنجا می‌ماند. هدف بازی این است که شما این ربات را آموزش دهید که تا حد ممکن در کوتاه‌ترین مسیر کلید را بردارد و به خانه‌‌ی گنج برود و بازی را به پایان برساند. پس از برداشتن کلید، علامت <code>-</code> به <code>+</code>  تغییر می‌کند. State بازی با\n",
    "<div dir=\"ltr\"> <center> <b> ((x,y),z) </b> </center></div> <br />  \n",
    "نشان داده می‌شود که (x,y) موقعیت ربات و z یک عدد باینری است که 0 به معنی این است که ربات هنوز کلید را برنداشته و 1 به معنی این است که کلید برداشته شده است.\n",
    "</font> </div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> محیط این بازی به کمک کلاس </font> </div>\n",
    "\n",
    "```python\n",
    "env = ENV(grid=(4,4), key=(0,2), coin=(2,3), holes=[(1,2),(1,3)], p=0.8, rewards='default')\n",
    "```\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "ساخته می‌شود. این کلاس مشخصات محیط را به عنوان وروی دریافت می‌کند که شامل سایز شبکه بازی، موقعیت کلید، موقعیت گنج، موقعیت گودال‌های مرگ، احتمال <code>p</code> و <code>rewards</code> بازی است. متغیر <code>rewards</code> مشخص می‌کند که محیط چه پاداشی را به ربات با وارد شدن به هر استیت می‌دهد. توجه داشته باشید تمام استیت‌های پایانی با احتمال 1 و پاداش 0 به خودشان انتقال پیدا می‌کنند. به طور پیش‌فرض پاداش همه‌ی استیت‌ها صفر است مگر استیتی که ربات در موقعیت گنج باشد و کلید را برداشته باشد که پاداش 1 دارد. برای تمامی قسمت‌های تمرین از مقادیر پیش‌فرض بازی (محیط رسم شده در شکل بالا و پاداش‌ پیش‌فرض) استفاده کنید، مگر سوال از شما بخواهد این مقادیر را تغییر دهید. این کلاس به عنوان خروجی متغیر و توابع زیر را در اختیار شما قرار می‌دهد:    \n",
    "</font> </div>\n",
    "<br />\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>env.action_space</code> فضای اکشن محیط </font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>env.state_space</code> فضای استیت محیط و مشخص کردن این که یک استیت نهایی هست یا خیر </font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>env.probability_space</code> توزیع  p(s’,r|s,a). به ازای هر (s,a) احتمال رخداد (s’,r) را به شما می‌دهد. <b>فقط در الگوریتم‌های model-based می‌توانید از این متغییر استفاده کنید</b>.\n",
    " </font> </div>\n",
    " \n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>()state = env.reset</code> با به طور تصادفی قرار گرفتن ربات در یکی از خانه‌ها بازی شروع می‌شود. </font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>state, reward, done = env.move(state, action)</code>  با گرفتن یک استیت و اکشن یک انتقال انجام می‌شود و استیت جدید، پاداش و این که این استیت نهایی هست یا خیر را در خروجی می‌دهد. <b> در صورت نیاز فقط در الگوریتم‌های model-based می‌توانید از این تابع استفاده کنید.\n",
    " </b></font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>state, reward, done = env.step(action)</code> یک اکشن در استیت فعلی بازی انجام می‌دهد. </font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>env.visual_screen(state)</code> یک تابع کمکی برای نمایش استیت بازی</font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>env.visual_state_date(data)</code> یک تابع کمکی برای نمایش value و policy همه‌ی استیت‌ها\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV:\n",
    "    def __init__(self, grid=(4,4), key=(0,2), coin=(2,3), holes=[(1,2),(1,3)], p=0.8, rewards='default'):\n",
    "        \n",
    "        '''\n",
    "        env = ENV()\n",
    "        Allowed variables and functions in the model-based problems:\n",
    "            env.action_space | env.state_space | env.probability_space\n",
    "            env.reset() | env.move() | env.step()\n",
    " \n",
    "         Allowed variables and functions in the model-free problems:\n",
    "            env.action_space | env.state_space\n",
    "            env.reset() | env.step()\n",
    "            \n",
    "        Visualization functions are always allowed:\n",
    "            env.visual_screen() | env.visual_state_date()\n",
    "        '''\n",
    "        \n",
    "        self.grid = grid\n",
    "        self.key = key\n",
    "        self.coin = coin\n",
    "        self.holes = holes\n",
    "        \n",
    "        all_loc = set(list(itertools.product(range(grid[0]), range(grid[1]))))\n",
    "        all_disallowed_loc = set([key, coin] + holes)\n",
    "        self.all_allowed_loc = all_loc - all_disallowed_loc\n",
    "        \n",
    "        self.action_space = {'R':(0,1), 'U':(-1,0), 'L':(0,-1), 'D':(1,0)}\n",
    "        self.state_space = {state:(state[0] in holes) or (state == (coin,1)) for state in list(itertools.product(list(all_loc),[0,1]))}\n",
    "        rewards = {state: 1.0 if state == (coin,1) else 0.0 for state in self.state_space.keys()} if rewards == 'default' else rewards\n",
    "        self.probability_space = {}\n",
    "        \n",
    "        def move_straight(state, action):\n",
    "            if self.state_space[state] == True:\n",
    "                return state\n",
    "            else:\n",
    "                old_loc, old_key = state\n",
    "                desired_loc = tuple(map(sum, zip(old_loc, self.action_space[action])))\n",
    "                new_loc = desired_loc if desired_loc in all_loc else old_loc\n",
    "                new_key = 1 if new_loc == key else old_key\n",
    "                new_state = (new_loc, new_key)\n",
    "                return new_state\n",
    "        wrong = {'R':'U', 'U':'L', 'L':'D', 'D':'R'}\n",
    "        for state, action in list(itertools.product(self.state_space.keys(), self.action_space.keys())):\n",
    "            state_straight = move_straight(state, action)\n",
    "            state_wrong = move_straight(state, wrong[action])\n",
    "            reward_state_straight = rewards[state_straight] if not self.state_space[state] else 0.0\n",
    "            reward_state_wrong = rewards[state_wrong] if not self.state_space[state] else 0.0\n",
    "            self.probability_space[(state, action)] = {(state_straight, reward_state_straight):p, (state_wrong, reward_state_wrong):1.0-p} if not(state_straight==state_wrong) else {(state_straight, reward_state_straight):1.0}\n",
    "            \n",
    "        self.state = None    \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        num = len(list(self.all_allowed_loc))\n",
    "        self.state = (list(self.all_allowed_loc)[np.random.choice(num)], 0)\n",
    "        return self.state\n",
    "    \n",
    "    def move(self, state, action):\n",
    "        num = len(self.probability_space[(state, action)].keys())\n",
    "        (next_state, reward) = list(self.probability_space[(state, action)].keys())[np.random.choice(num,p=list(self.probability_space[(state, action)].values()))]\n",
    "        return next_state, reward, self.state_space[next_state]\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state, reward, done = self.move(self.state, action)\n",
    "        return self.state, reward, done\n",
    "    \n",
    "    def visual_screen(self, state):\n",
    "        for i in range(self.grid[0]):\n",
    "            print(\"--------\" * self.grid[1])\n",
    "            for j in range(self.grid[1]):\n",
    "                x = \"A\" if (i,j) == state[0] else \" \"\n",
    "                if (i,j) == self.key and state[1] == 0: y = \"-\"\n",
    "                elif (i,j) == self.key and state[1] == 1: y = \"+\"\n",
    "                elif (i,j) == self.coin: y = \"$\"\n",
    "                elif (i,j) in self.holes: y = \"#\"\n",
    "                else: y = \" \"\n",
    "                print(\"   {:.2}  |\".format(x+y), end=\"\")\n",
    "            print(\"\")\n",
    "\n",
    "    def visual_state_date(self, data):\n",
    "        strtype = data[((0,0),0)] in ['R','U','L','D']\n",
    "        for i in range(self.grid[0]):\n",
    "            print(2*(\"--------\" * self.grid[1] + \"        \"))\n",
    "            for k in range(2):\n",
    "                for j in range(self.grid[1]):\n",
    "                    x = data[((i,j),k)]\n",
    "                    if (i,j) == self.key and k == 0: y = \"-\"\n",
    "                    elif (i,j) == self.key and k == 1: y = \"+\"\n",
    "                    elif (i,j) == self.coin: y = \"$\"\n",
    "                    elif (i,j) in self.holes: y = \"#\"\n",
    "                    else: y = \" \"\n",
    "                    if strtype:\n",
    "                        print(\"   {}{}  |\".format(x,y), end=\"\")\n",
    "                    elif x >= 0:\n",
    "                        print(\" {:.2f} {}|\".format(x,y), end=\"\")\n",
    "                    else:\n",
    "                        print(\"{:.2f} {}|\".format(x,y), end=\"\")\n",
    "                print(\"        \".format(x,y), end=\"\")\n",
    "            print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. An experience with the environment\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "    در این قسمت می‌خواهیم با محیط بازی و توابع آن آشنا شویم.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Cell های زیر به منظور آشنایی شما با محیط بازی نوشته شده‌اند. آن‌ها را اجرا کنید.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intro\n",
    "env = ENV(p=0.8)\n",
    "\n",
    "# env.state_space and env.action_space\n",
    "print('env.state_space =', env.state_space, '\\n')\n",
    "print('env.action_space =',env.action_space, '\\n')\n",
    "\n",
    "# set a random value\n",
    "value = {state:np.random.rand() for state in env.state_space}\n",
    "\n",
    "# set a random policy\n",
    "policy = {state:np.random.choice(list(env.action_space.keys())) for state in env.state_space}\n",
    "\n",
    "# visual value\n",
    "env.visual_state_date(value)\n",
    "print(\"\")\n",
    "\n",
    "# visual policy\n",
    "env.visual_state_date(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL a policy in a game\n",
    "def EVAL(env, policy, max_step_per_episode=100):\n",
    "    states_track = []\n",
    "    rewards_track = []\n",
    "    \n",
    "    # game loop\n",
    "    state = env.reset(); states_track.append(state)\n",
    "    for num_step in range(max_step_per_episode):\n",
    "        state, reward, done = env.step(policy[state]); states_track.append(state); rewards_track.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return states_track, sum(rewards_track), num_step+1\n",
    "\n",
    "# visualize game by gif\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def gif(env, states_track, delay):\n",
    "    for state in states_track:\n",
    "        clear_output(wait=True)\n",
    "        env.visual_screen(state)\n",
    "        sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL a random policy\n",
    "env = ENV(p=0.8)\n",
    "states_track, reward_sum, num_step = EVAL(env, policy)\n",
    "print(reward_sum, num_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif\n",
    "gif(env, states_track, 10/num_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Code\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "    کد کلاس <code>ENV</code> را بررسی کنید و به طور مختصر درباره‌ی چگونگی پیاده‌سازی آن توضیح دهید.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Policy\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "   Cell زیر را اجرا کنید و خروجی آن را آنالیز کنید.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL the random policy\n",
    "env = ENV(p=0.8)\n",
    "\n",
    "num_test = 10000\n",
    "total_reward = 0.0\n",
    "for t in range(num_test):\n",
    "    policy = {state:np.random.choice(list(env.action_space.keys())) for state in env.state_space}\n",
    "    states_track, reward_sum, num_step = EVAL(env, policy); total_reward += reward_sum\n",
    "print(total_reward / num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Policy Iteration\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "در این قسمت می‌خواهیم الگوریتم Policy Iteration را پیاده‌سازی کنیم. اساس این الگوریتم به صورت زیر است:\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The used Bellman equations:**\n",
    "\n",
    "Policy evaluation (Recursive for $V$ in $\\pi$):\n",
    "\n",
    "$V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\Bigg(\\sum_{s' \\in S} p(s'|s,a) \\Big(r(s,a,s') + \\gamma V_{\\pi}(s') \\Big) \\Bigg)$\n",
    "\n",
    "Policy improvement (For $V$ in $\\pi_{*}$):\n",
    "\n",
    "$\\pi_{*}(s) = \\arg\\max_a \\sum_{s' \\in S} p(s'|s,a) \\Big(r(s,a,s') + \\gamma V_{*}(s') \\Big)$ \n",
    "\n",
    "Note ([Wikipedia]): For all final states $s_{f}$, $V(s_{f})$ and $Q(s_{f},a)$ are never updated, but is set to the reward value $r$ observed for state $s_{f}$. In most cases, $V(s)$ and $Q(s_{f},a)$ can be taken to equal zero.\n",
    "\n",
    "[Wikipedia]: https://en.wikipedia.org/wiki/Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial value of final states\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "چرا در الگوریتم‌های براساس bootstrapping باید مقادیر اولیه <code>value</code> در استیت‌های نهایی برابر صفر تنظیم گردد؟\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Guess the optimal policy\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">قبل از پیاده‌سازی الگوریتم، پیش‌بینی کنید که <code>policy</code> بهینه به ازای <code>p=[0.2, 0.6, 0.8, 1.0]</code> چگونه خواهد بود؟ نیاز نیست پاسخ شما دقیق و منطبق بر نتایج شبیه‌سازی باشد ولی مستدل آن را بیان کنید.\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Policy Iteration Algorithm\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "الگوریتم Policy Iteration را پیاده‌سازی کنید:\n",
    "</font> </div>\n",
    "\n",
    "\n",
    "```python\n",
    "value, policy, value_track, policy_track, num_iter = Train_PolicyIteration(env, GAMMA=0.9, SMALL_ENOUGH=1e-3)\n",
    "```\n",
    "\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "این تابع به عنوان ورودی محیط بازی، گاما و استانه توقف Policy Evaluation را دریافت می‌کند و در خروجی <code>value</code> بهینه، <code>policy</code> بهینه، لیست تاریخچه <code>value</code>  به روز شده بعد هر بار اجرای Policy Evaluation، لیست تاریخچه <code>policy</code>  به روز شده بعد هر بار اجرای Policy Improvement و تعداد تکرارهای انجام شده (هر به روزرسانی در <code>value</code> یا <code>policy</code> یک استیت یک تکرار است)  را می‌دهد.\n",
    " </font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration Alg.\n",
    "\n",
    "def Train_PolicyIteration(env, GAMMA=0.9, SMALL_ENOUGH=1e-3):\n",
    "    value_track = []\n",
    "    policy_track = []\n",
    "    num_iter = 0\n",
    "\n",
    "    # Initialization --------------------------------------------------\n",
    "    # set a random value\n",
    "    value = None # TODO\n",
    "    # set a random policy\n",
    "    policy = None # TODO\n",
    "    # track\n",
    "    value_track.append(value.copy()); policy_track.append(policy.copy())\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation --------------------------------------------------\n",
    "        # TODO\n",
    "        \n",
    "        # Policy Improvement --------------------------------------------------\n",
    "        # TODO\n",
    "\n",
    "        # track\n",
    "        value_track.append(value.copy()); policy_track.append(policy.copy())\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return value, policy, value_track, policy_track, num_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze the results\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Cell زیر را اجرا کنید و به طور مفصل نتایج آن (همگرایی، تعداد تکرار، <code>value</code>  بهینه و <code>policy</code> بهینه در هر <code>p</code>) را تجزیه و تحلیل کنید.\n",
    "</font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> به طور خاص\n",
    "به ازای <code>p</code> های مختلف، نتایج <code>policy</code> بهینه در استیت‌های    زیر را آنالیز کنید.\n",
    "</font> </div>  \n",
    "\n",
    "```python\n",
    "States: ((0,2),0), ((0,2),1), ((1,1),1)\n",
    "```\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "به صورت دستی برای <code>policy</code> های بهینه در <code>p</code> های مختلف، مقدار <code>value</code> در استیت‌های  زیر را به دست آورید و با نتیجه‌ی شبیه‌سازی مقایسه کنید.\n",
    "</font> </div>\n",
    "\n",
    "```python\n",
    "States: ((3,3),1), ((2,2),1), ((3,2),1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Alg.\n",
    "p_list = [1.0, 0.8, 0.6, 0.2]\n",
    "GAMMA = 0.9; SMALL_ENOUGH = 1e-3\n",
    "\n",
    "\n",
    "num_test = 100\n",
    "for p in p_list:\n",
    "    \n",
    "    # for a probability p\n",
    "    env = ENV(p=p)\n",
    "    opt_value_track = []\n",
    "    number_of_iteration = 0\n",
    "    for t in range(num_test):\n",
    "        value, policy, value_track, policy_track, num_iter = Train_PolicyIteration(env, GAMMA, SMALL_ENOUGH)\n",
    "        opt_value_track.append(value)\n",
    "        number_of_iteration += num_iter\n",
    "    \n",
    "    # visual\n",
    "    print(f\"p = {p} \"+90*'~')\n",
    "    print(f\"convergence = {all(((np.array(list(element.values())) - np.array(list(opt_value_track[0].values())))**2).mean() <= 1e-4 for element in opt_value_track)} | number of iteration = {number_of_iteration / num_test}\")\n",
    "    env.visual_state_date(value)\n",
    "    env.visual_state_date(policy)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot $V_{\\pi_k}(s)$ vs $k$\n",
    "\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "الگوریتم خود را برای محیط با پارامتر <code>p=0.8</code> اجرا کنید. نمودار مقدار <code>value</code> استیت‌های زیر را در Initialization و بعد اجرای هر بار Policy Evaluation رسم کنید. همه‌ی نمودار‌ها را در یک plot رسم کنید. نتایج را آنالیز کنید.\n",
    "</font> </div>\n",
    "\n",
    "```python\n",
    "States: ((3,0),0), ((2,1),0), ((0,2),1), ((1,1),1), ((2,2),1)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Cell زیر عملکرد الگوریتم شما را در یک بازی نشان می‌دهد.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif\n",
    "GAMMA = 0.9; SMALL_ENOUGH = 1e-3\n",
    "\n",
    "env = ENV(p=0.8)\n",
    "_, policy, _, _, _= Train_PolicyIteration(env, GAMMA, SMALL_ENOUGH)\n",
    "states_track, reward_sum, num_step = EVAL(env, policy)\n",
    "\n",
    "gif(env, states_track, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Effects of the discount factor $\\gamma$\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Cell زیر تاثیر انتخاب مقدار <code>GAMMA</code> بر <code>policy</code> بهینه در محیط‌های مختلف را بررسی می‌کند. نتایج را آنالیز کنید.\n",
    "    <br />\n",
    "چرا نتایج تقارن ندارد؛ یعنی عملکرد Agent در <code>ENV(p=p)</code> و <code>ENV(p=1-p)</code> یکسان نیست.\n",
    "     <br />\n",
    " <code>cumulative reward</code>  یا پاداش تجمعی در یک episode به معنی جمع تمام پاداش‌ها بدون احتساب گاما است.   \n",
    " </font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL the optimal policy\n",
    "p_list2 = np.linspace(0.,1.,50)\n",
    "GAMMA_list = [0.2, 0.5, 0.9, 0.99]; SMALL_ENOUGH = 1e-3\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "for GAMMA in GAMMA_list:\n",
    "    r_list = []\n",
    "    n_list = []\n",
    "    for p in p_list2:\n",
    "        env = ENV(p=p)\n",
    "        _, policy, _, _, _= Train_PolicyIteration(env, GAMMA, SMALL_ENOUGH)\n",
    "\n",
    "        num_test = 1000\n",
    "        total_reward = 0.0\n",
    "        total_num_step = 0\n",
    "        for t in range(num_test):\n",
    "            states_track, reward_sum, num_step = EVAL(env, policy); total_reward += reward_sum; total_num_step += num_step;\n",
    "        r_list.append(total_reward / num_test)\n",
    "        n_list.append(total_num_step / num_test)\n",
    "\n",
    "    ax[0].plot(p_list2, r_list, label=f\"GAMMA = {GAMMA}\")\n",
    "    ax[1].plot(p_list2, n_list, label=f\"GAMMA = {GAMMA}\")\n",
    "\n",
    "ax[0].set_xlabel('p')\n",
    "ax[0].set_ylabel('the average cumulative reward per episode')\n",
    "ax[1].set_xlabel('p')\n",
    "ax[1].set_ylabel('the average number of steps per episode')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Case $\\gamma = 1$\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "کارکرد الگوریتم خود را برای <code>GAMMA = 1</code> بررسی کنید. اگر الگوریتم شما همگرا نمی‌شود دلیل آن را ذکر کنید.\n",
    " </font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the true value to compare with other algorithms\n",
    "p_list = [1.0, 0.8, 0.6, 0.2]\n",
    "GAMMA = 0.9; SMALL_ENOUGH = 1e-3\n",
    "\n",
    "save_value = {p: Train_PolicyIteration(ENV(p=p), GAMMA, SMALL_ENOUGH)[0] for p in p_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Value Iteration\n",
    "\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "در این قسمت می‌خواهیم الگوریتم Value Iteration را پیاده‌سازی کنیم. اساس این الگوریتم به صورت زیر است:\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The used Bellman equation:**\n",
    "\n",
    "Recursive for $V$ in $\\pi_{*}$:\n",
    "\n",
    "$V_{*}(s) = \\max_a \\sum_{s' \\in S} p(s'|s,a) \\Big(r(s,a,s') + \\gamma V_{*}(s') \\Big)$\n",
    "\n",
    "Note ([Wikipedia]): For all final states $s_{f}$, $V(s_{f})$ and $Q(s_{f},a)$ are never updated, but is set to the reward value $r$ observed for state $s_{f}$. In most cases, $V(s)$ and $Q(s_{f},a)$ can be taken to equal zero.\n",
    "\n",
    "[Wikipedia]: https://en.wikipedia.org/wiki/Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement Value Iteration Algorithm\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "الگوریتم Value Iteration را پیاده‌سازی کنید:\n",
    "</font> </div>\n",
    "\n",
    "```python\n",
    "value, policy, value_track, num_iter = Train_ValueIteration(env, GAMMA=0.9, SMALL_ENOUGH=1e-3)\n",
    "```\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "این تابع به عنوان ورودی محیط بازی، گاما و استانه توقف الگوریتم را دریافت می‌کند و در خروجی <code>value</code>  بهینه، <code>policy</code> بهینه، لیست تاریخچه <code>value</code>  به روز شده بعد هر بار loop  بر روی تمام استیت‌ها و تعداد تکرارهای انجام شده (هر به روزرسانی در <code>value</code> یک استیت یک تکرار است) را می‌دهد.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Alg.\n",
    "\n",
    "def Train_ValueIteration(env, GAMMA=0.9, SMALL_ENOUGH=1e-3):\n",
    "    value_track = []\n",
    "    num_iter = 0\n",
    "\n",
    "    # Initialization --------------------------------------------------\n",
    "    # set a random value\n",
    "    value = None # TODO\n",
    "    # track\n",
    "    value_track.append(value.copy())\n",
    "\n",
    "    # Value Iteration --------------------------------------------------\n",
    "    # TODO\n",
    "\n",
    "    # Compute a deterministic optimal policy ---------------------------\n",
    "    # TODO\n",
    "\n",
    "    return value, policy, value_track, num_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze the results\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Cell زیر را اجرا کنید و نتایج آن (همگرایی، تعداد تکرار، <code>value</code>  بهینه و <code>policy</code> بهینه در هر <code>p</code>) را با الگوریتم Policy Iteration مقایسه کنید.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Alg.\n",
    "p_list = [1.0, 0.8, 0.6, 0.2]\n",
    "GAMMA = 0.9; SMALL_ENOUGH = 1e-3\n",
    "\n",
    "\n",
    "num_test = 100\n",
    "for p in p_list:\n",
    "    \n",
    "    # for a probability p\n",
    "    env = ENV(p=p)\n",
    "    opt_value_track = []\n",
    "    number_of_iteration = 0\n",
    "    for t in range(num_test):\n",
    "        value, policy, value_track, num_iter = Train_ValueIteration(env, GAMMA, SMALL_ENOUGH)\n",
    "        opt_value_track.append(value)\n",
    "        number_of_iteration += num_iter\n",
    "    \n",
    "    # visual\n",
    "    print(f\"p = {p} \"+90*'~')\n",
    "    print(f\"convergence = {all(((np.array(list(element.values())) - np.array(list(save_value[p].values())))**2).mean() <= 1e-4 for element in opt_value_track)} | number of iteration = {number_of_iteration / num_test}\")\n",
    "    env.visual_state_date(value)\n",
    "    env.visual_state_date(policy)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot $V_{\\pi_k}(s)$ vs $k$\n",
    "\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "الگوریتم خود را برای محیط با پارامتر <code>p=0.8</code> اجرا کنید. نمودار مقدار <code>value</code> استیت‌های زیر را در Initialization و بعد اجرای هر بار loop بر روی تمام استیت‌ها رسم کنید. همه‌ی نمودار‌ها را در یک plot رسم کنید. نتایج را آنالیز و با نمودار مشابه در قسمت Policy Iteration مقایسه کنید.\n",
    "</font> </div>\n",
    "\n",
    "```python\n",
    "States: ((3,0),0), ((2,1),0), ((0,2),1), ((1,1),1), ((2,2),1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Q-Learning\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "در این قسمت می‌خواهیم الگوریتم Q-Learning را پیاده‌سازی کنیم. اساس این الگوریتم به صورت زیر است:\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The used Bellman equation:**\n",
    "\n",
    "Recursive for $Q$ in $\\pi_{*}$:\n",
    "\n",
    "$Q_{*}(s,a) \\overset{model-based}{=} \\sum_{s' \\in S} p(s'|s,a) \\Big(r(s,a,s') + \\gamma \\max_{a'}Q_*(s',a') \\Big)  \\overset{model-free}{=} \\mathbb{E} [R + \\gamma \\max_{a'}Q_*(S',a')]$\n",
    "\n",
    "\n",
    "The agent doesn't know $p(s'|s,a)$ and $r(s',s,a)$.\n",
    "\n",
    "[Wikipedia]: The agent receives $s_t$, then chooses an action $a_{t}$. The environment moves to a new state $s_{t+1}$ and the reward $r_{t+1}$ associated with the transition $(s_{t},a_{t},s_{t+1})$ is determined. The goal of reinforcement learning is to learn a policy: $\\pi :A\\times S\\rightarrow [0,1]$, $\\pi (a,s)=\\Pr(a_{t}=a\\mid s_{t}=s)$ which maximizes the expected cumulative reward. The core of the algorithm is a Bellman equation as a simple value iteration update, using the weighted average of the old value and the new information:\n",
    "\n",
    "[Wikipedia]: https://en.wikipedia.org/wiki/Q-learning\n",
    "\n",
    "\n",
    "$ Q^{new}(s_{t},a_{t})\\leftarrow \\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}+\\underbrace {\\alpha } _{\\text{learning rate}}\\cdot \\overbrace {{\\bigg (}\\underbrace {\\underbrace {r_{t+1}} _{\\text{reward}}+\\underbrace {\\gamma } _{\\text{discount factor}}\\cdot \\underbrace {\\max _{a}Q(s_{t+1},a)} _{\\text{estimate of optimal future value}}} _{\\text{new value (temporal difference target)}}-\\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}{\\bigg )}} ^{\\text{temporal difference}}$\n",
    "\n",
    "How to update $Q(s,a)$?\n",
    "\n",
    "**1) Exploration and then Exploitation:**\n",
    "\n",
    "**By Loop:**\n",
    "```python\n",
    "init Q \n",
    "# For all final states s_f in final_states, Q(s_f,a) is never updated, but is set to the reward value r observed for state s_f. \n",
    "# In most cases, Q(s_f,a) can be taken to equal zero. \n",
    "\n",
    "while doesnt converge:\n",
    "  for s in S:\n",
    "    for a in A:\n",
    "      # the environment reaction\n",
    "      s_new, r = env.move(s,a)\n",
    "      # update Q(s,a)\n",
    "      Q(s,a) += alpha * (r + gamma * max(Q(s_new,:)) - Q(s,a))\n",
    "```\n",
    "\n",
    "\n",
    "**2) Exploration and Exploitation, at the same time:**\n",
    "\n",
    "**Play a game!**\n",
    "```python\n",
    "init Q \n",
    "# For all final states s_f in final_states, Q(s_f,a) is never updated, but is set to the reward value r observed for state s_f. \n",
    "# In most cases, Q(s_f,a) can be taken to equal zero. \n",
    "\n",
    "while doesnt converge:\n",
    "  # play a game!\n",
    "  s = random start state\n",
    "  # a <- policy(Q(s,:)) can be random policy or epsilon-greedy or softmax, etc.\n",
    "  while not s in final_states:\n",
    "    # the agent action in state s\n",
    "    a = policy(Q(s,:))\n",
    "    # the environment reaction\n",
    "    s_new, r = env.step(a)\n",
    "    # update Q(s,a)\n",
    "    Q(s,a) += alpha * (r + gamma * max(Q(s_new,:)) - Q(s,a))\n",
    "    # update state\n",
    "    s = s_new\n",
    "```\n",
    "\n",
    "Indeed, `policy(Q(s,:))` in Q-Learning is *the behavior policy*, not the target policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement Q-Learning Algorithm\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "الگوریتم Q-Learning را به صورت کاوش و بهره‌برداری همزمان پیاده‌سازی کنید:\n",
    "</font> </div>\n",
    "\n",
    "```python\n",
    "value, policy, reward_episode_track, i_step_episode_track, final_state_track = Train_QLearning(env, GAMMA=0.9, ALPHA=0.1, max_step_per_episode=100, eps_max=1.0, eps_min=0.0, eps_num_explore=1000, eps_num_explore_exploit=8000, max_num_episode=1000)\n",
    "```\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "این تابع به عنوان ورودی محیط بازی، گاما، نرخ یادگیری، حداکثر تعداد step برای هرepisode، حداکثر مقدار epsilon، حداقل مقدار epsilon، تعداد step ای که epsilon بر روی مقدار حداکثر خود است، تعداد step ای که epsilon به صورت خطی از مقدار بیشینه به کمینه می‌رسد و تعداد episode ای که در الگوریتم اجرا می‌شود را دریافت می‌کند و در خروجی <code>value</code>  بهینه، <code>policy</code> بهینه، لیست reward تجمعی هر episode، لیست شماره‌ی step ای که هر episode در آن پایان یافته و لیست استیتی که هر episode با آن پایان یافته است را می‌دهد.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning Alg.\n",
    "\n",
    "def Train_QLearning(env, GAMMA=0.9, ALPHA=0.1, max_step_per_episode=100, eps_max=1.0, eps_min=0.0, eps_num_explore=1000, eps_num_explore_exploit=8000, max_num_episode=1000):\n",
    "    reward_episode_track = []\n",
    "    i_step = 0\n",
    "    i_step_episode_track = []\n",
    "    final_state_track = []\n",
    "\n",
    "    # Initialization --------------------------------------------------\n",
    "    # set a random Q \n",
    "    Q = None # TODO\n",
    "\n",
    "    for _ in range(max_num_episode):\n",
    "        # Play a game! --------------------------------------------------\n",
    "        state = env.reset()\n",
    "        reward_episode_sum = 0.0\n",
    "        for num_step in range(max_step_per_episode):\n",
    "            i_step += 1\n",
    "            \n",
    "            # ---------------------- choose action ----------------------\n",
    "            # update epsilon\n",
    "            # TODO\n",
    "            # epsilon greedy\n",
    "            # TODO\n",
    "            \n",
    "            # ---------------------- do the step ----------------------\n",
    "            # TODO\n",
    "            reward_episode_sum += reward\n",
    "            \n",
    "            # ------------------------ update Q ------------------------\n",
    "            # TODO\n",
    "            \n",
    "            # -------------------- go to next_state --------------------\n",
    "            # TODO\n",
    "            \n",
    "            # ------------------------- if done -------------------------\n",
    "            if done:\n",
    "                break\n",
    "        reward_episode_track.append(reward_episode_sum)\n",
    "        i_step_episode_track.append(i_step + 1)\n",
    "        final_state_track.append(state)\n",
    "    \n",
    "    # Compute optimal values and a deterministic optimal policy --------------------------------------------------\n",
    "    # TODO\n",
    "    \n",
    "    return value, policy, reward_episode_track, i_step_episode_track, final_state_track\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze the results\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> Cell\n",
    " زیر را اجرا کنید و به طور مفصل نتایج آن (همگرایی، تعداد تکرار، reward  تجمعی هر episode، تعداد step هر episode، مقدار <code>value</code>  بهینه و <code>policy</code> بهینه در هر <code>p</code>) را تجزیه و تحلیل و با الگوریتم‌های قبلی مقایسه کنید.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Eval Alg.\n",
    "p_list = [1.0, 0.8, 0.6, 0.2]\n",
    "GAMMA=0.9; ALPHA=0.1; max_step_per_episode=100; eps_max=1.0; eps_min=0.0; eps_num_explore=1000; eps_num_explore_exploit=8000; max_num_episode=1000\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "num_test = 20\n",
    "for p in p_list:\n",
    "    \n",
    "    # for a probability p\n",
    "    env = ENV(p=p)\n",
    "    opt_value_track = []\n",
    "    reward_episode_track_sum = np.zeros(max_num_episode)\n",
    "    num_step_episode_track_sum = np.zeros(max_num_episode)\n",
    "    number_of_iteration = 0\n",
    "    for t in range(num_test):\n",
    "        value, policy, reward_episode_track, i_step_episode_track, final_state_track = Train_QLearning(env, GAMMA, ALPHA, max_step_per_episode, eps_max, eps_min, eps_num_explore, eps_num_explore_exploit, max_num_episode)\n",
    "        opt_value_track.append(value)\n",
    "        reward_episode_track_sum += np.array(reward_episode_track)\n",
    "        num_step_episode_track_sum += np.array(i_step_episode_track) - np.array([0]+i_step_episode_track[:-1])\n",
    "        number_of_iteration += i_step_episode_track[-1]\n",
    "    \n",
    "    # visual\n",
    "    print(f\"p = {p} \"+90*'~')\n",
    "    MSE = sum(((np.array(list(element.values())) - np.array(list(save_value[p].values())))**2).mean() for element in opt_value_track)/len(opt_value_track)\n",
    "    print(\"MSE convergence = {:.5f} | number of iteration = {}\".format(MSE,number_of_iteration/len(opt_value_track)))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax[0].plot(range(max_num_episode-9), moving_average(reward_episode_track_sum/num_test, 10), label=f\"p = {p}\")\n",
    "    ax[1].plot(range(max_num_episode-9), moving_average(num_step_episode_track_sum/num_test, 10), label=f\"p = {p}\")\n",
    "    ax[0].set_xlabel('episode')\n",
    "    ax[0].set_ylabel('the average cumulative reward per episode')\n",
    "    ax[1].set_xlabel('episode')\n",
    "    ax[1].set_ylabel('the average number of steps per episode')\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "    env.visual_state_date(value)\n",
    "    env.visual_state_date(policy)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Cell زیر عملکرد الگوریتم شما را در یک بازی نشان می‌دهد.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif\n",
    "GAMMA=0.9; ALPHA=0.1; max_step_per_episode=100; eps_max=1.0; eps_min=0.0; eps_num_explore=1000; eps_num_explore_exploit=8000; max_num_episode=1000\n",
    "\n",
    "env = ENV(p=0.8)\n",
    "value, policy, _, _, _ = Train_QLearning(env, GAMMA, ALPHA, max_step_per_episode, eps_max, eps_min, eps_num_explore, eps_num_explore_exploit, max_num_episode)\n",
    "states_track, reward_sum, num_step = EVAL(env, policy)\n",
    "\n",
    "gif(env, states_track, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Effects of $\\alpha$ and $\\varepsilon$\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "    تاثیر پارامتر‌های <code>ALPHA</code> و epsilon های مختلف را بر کارکرد الگوریتم بررسی کنید.\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using random policy\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "در کلاس درس شما با الگوریتم Q-Learning به صورتی آشنا شدید که  Agent برای انتخاب اکشن در هر گام از epsilon-greedy-policy استفاده می‌کند. اگر ما به جای epsilon-greedy-policy از random-policy (یعنی در هر گام کاملا تصادفی و بدون توجه به مقادیر value یک اکشن انتخاب شود) استفاده کنیم، آیا الگوریتم می‌تواند به مقادیر Q بهینه همگرا شود؟ در مورد الگوریتم SARSA چطور؟ توضیح دهید.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design $r(s)$ and $\\gamma$\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "    در <code>GAMMA = 0.9</code> متغیر <code>rewards</code> بازی را طوری تغییر دهید که الگوریتم‌های پیاده‌سازی شده عملکرد بهتری داشته باشند. در اینجا عملکرد یک الگوریتم را از دو نظر بررسی کنید.\n",
    "<br />\n",
    "۱) همگرایی آن‌ها به جواب سریع‌تر گردد.    \n",
    "<br />    \n",
    " ۲) در طول زمان training عملکرد Agent از نظر احتمال رسیدن آن به گنج بهتر باشد.\n",
    "این عملکرد در الگوریتم Policy Iteration از ارزیابی <code>policy_track</code> در خروجی، در الگوریتم Value Iteration از ارزیابی <code>value_track</code> در خروجی و در الگوریتم Q-Learning از <code>final_state_track</code> در خروحی قابل بررسی است.\n",
    "<br />   \n",
    "    حال چند سناریو مختلف برای <code>rewards</code> در نظر بگیرید و نتایج را به ازای <code>GAMMA</code> های مختلف از جمله <code>GAMMA = 1</code> مقایسه و آنالیز کنید.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Design a fun map!\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "    به دلخواه خود یک نقشه‌ی گنج جالب طراحی کنید و ربات را با الگوریتم Q-Learning آموزش دهید. با <code>gif</code> عملکرد ربات را نشان دهید.\n",
    " </font> </div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give Us Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145a46d259b94d49be1408277befd7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='How was the homework?'),)), HBox(children=(Label(value='Please rate…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Label, IntSlider, Textarea, Button, Layout, HBox, VBox\n",
    "from IPython.display import clear_output\n",
    "\n",
    "IntSlider_list = {i:IntSlider(value=3, min=1, max=5) for i in range(6)}\n",
    "H1 = HBox([Label(\"How was the homework?\")])\n",
    "H2 = HBox([Label(\"Please rate the homework from 1 to 5 (lowest to highest).\")])\n",
    "H3 = HBox([Label(\"The assignment was instructive.\", layout=Layout(width=\"22%\")), IntSlider_list[0]])\n",
    "H4 = HBox([Label(\"It was not time consuming.\", layout=Layout(width=\"22%\")), IntSlider_list[1]])\n",
    "H5 = HBox([Label(\"The questions were clear.\", layout=Layout(width=\"22%\")), IntSlider_list[2]])\n",
    "H6 = HBox([Label(\"The environment was interesting.\", layout=Layout(width=\"22%\")), IntSlider_list[3]])\n",
    "H7 = HBox([Label(\"The homework was well organized.\", layout=Layout(width=\"22%\")), IntSlider_list[4]])\n",
    "H8 = HBox([Label(\"Finally, you had a good vibe!\", layout=Layout(width=\"22%\")), IntSlider_list[5]])\n",
    "textarea = Textarea(value='', placeholder='Any comments or suggestions', description='', disabled=False)\n",
    "button = Button(description='Submit', disabled=False, button_style='', tooltip='Click me', icon='paper-plane')\n",
    "H9 = HBox([textarea])\n",
    "H10 = HBox([button])\n",
    "\n",
    "def ff(button):\n",
    "    button.button_style = 'success'\n",
    "    score = sum([IntSlider_list[i].value for i in IntSlider_list]) / 6\n",
    "    x = round(score*4)/4\n",
    "    y = int(x) * '🌕' + ((x-int(x))==0 and not x==5) * '🌑' + ((x-int(x))==0.25) * '🌘' + ((x-int(x))==0.5) * '🌗' + ((x-int(x))==0.75) * '🌖' + (4-int(x)) * '🌑'\n",
    "    clear_output(wait=True)\n",
    "    display(VBox([H1, H2, H3, H4, H5, H6, H7, H8, H9, H10]))\n",
    "    print(\"Thank you for your feedback! 😊\")\n",
    "    print(f\"Score: {y}\")\n",
    "    print(\"\\nFeedback:\",*[IntSlider_list[i].value for i in IntSlider_list])\n",
    "    print(textarea.value)\n",
    "\n",
    "button.on_click(ff)\n",
    "\n",
    "VBox([H1, H2, H3, H4, H5, H6, H7, H8, H9, H10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To export jupyter notebook to html, save your notebook and run this cell\n",
    "!jupyter nbconvert --to html CHW1.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
