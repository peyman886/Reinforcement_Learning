{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" class=\"alert alert-block alert-info\"> <font size=\"5\" face=\"HM XNiloofar\"> \n",
    "<b> ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ - Ø¯Ú©ØªØ± Ú¯Ù„Ø³ØªØ§Ù†ÛŒ: ØªÙ…Ø±ÛŒÙ† Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ </b>\n",
    "</font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "<br />\n",
    "Ù…Ù‡Ù„Øª ØªØ­ÙˆÛŒÙ„ ØªÙ…Ø±ÛŒÙ†: Û¶ Ø®Ø±Ø¯Ø§Ø¯ Û±Û´Û°Û± <br />\n",
    "ÙÙ‚Ø· Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§ÛŒ <code>TODO</code> Ø±Ø§ Ù¾Ø± Ú©Ù†ÛŒØ¯ Ùˆ  Jupyter Notebook ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª <code>ipynb</code> Ùˆ <code>html</code> Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø¯Ø± CW Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯. <br />\n",
    "Ø§Ø¨Ù‡Ø§Ù…Ø§Øª Ùˆ Ø³ÙˆØ§Ù„Ø§Øª Ø®ÙˆØ¯ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø±Ø§ Ù…ÛŒ ØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ Ø·Ø±Ø§Ø­ ØªÙ…Ø±ÛŒÙ† Ù…Ø·Ø±Ø­ Ú©Ù†ÛŒØ¯. <br />\n",
    "<div dir=\"ltr\">@hamidreza_ehteram</div> <br />  \n",
    "Ø¯Ø± Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¯Ø± ÛŒÚ© Ù…Ø­ÛŒØ· Ø¨Ø§Ø²ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Policy Iteration   Ùˆ Value Iteration Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¯Ùˆ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… model-based Ùˆ Q-Learning Ø±Ø§ Ø¨Ø±Ø§ÛŒ model-free  Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ Ùˆ Ø¨Ø§ ØªØ§Ø«ÛŒØ± Ù¾Ø§Ø±Ø§Ù…ØªØ±â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. <br />\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***You can jump directly to these sections:***\n",
    "\n",
    "+ [A. An experience with the environment](#A.-An-experience-with-the-environment)\n",
    "+ [B. Policy Iteration (model-based)](#B.-Policy-Iteration)\n",
    "+ [C. Value Iteration (model-based)](#C.-Value-Iteration)\n",
    "+ [D. Q-Learning (model-free)](#D.-Q-Learning)\n",
    "+ [Give Us Feedback](#Give-Us-Feedback)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> <b>  Ø¢Ø®Ø±ÛŒÙ† Ú¯Ù†Ø¬ Ù‚Ø§Ø±ÙˆÙ†: </b>\n",
    " ÛŒÚ© ØªÛŒÙ… Ú©Ø§ÙˆØ´Ú¯Ø±ÛŒ Ø¯Ø± ÛŒÚ©ÛŒ Ø§Ø² Ù…Ù†Ø§Ø·Ù‚ Ø¨Ø§Ø³ØªØ§Ù†ÛŒØŒ Ù†Ù‚Ø´Ù‡â€ŒÛŒ Ú¯Ù†Ø¬ÛŒ Ú©Ù‡ Ø¯Ø± Ø²ÛŒØ±Ø²Ù…ÛŒÙ†ÛŒ Ù…ØªØ±ÙˆÚ©Ù‡ Ù…Ø®ÙÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¯Ø± Ù†Ù‚Ø´Ù‡ Ù…Ø­Ù„ Ú¯Ù†Ø¬ ğŸ’°ØŒ Ú©Ù„ÛŒØ¯ Ø¢Ù† ğŸ”‘ Ùˆ Ú¯ÙˆØ¯Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ú¯ ğŸ•³ï¸ Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡  Ø§Ø³Øª. Ø§Ø² Ø¢Ù†Ø¬Ø§ Ú©Ù‡ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø§ÛŒÙ† Ú¯Ù†Ø¬ ÙÙ‚Ø· Ø¨Ù‡ Ú©Ù…Ú© ÛŒÚ© Ø±Ø¨Ø§Øª Ú©Ø§ÙˆØ´Ú¯Ø± ğŸ¤– Ù…Ù…Ú©Ù† Ø§Ø³ØªØŒ Ø§ÛŒÙ† ØªÛŒÙ… Ø§Ø² Ø´Ù…Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù…ØªØ®ØµØµ Ø¹Ù„ÙˆÙ… Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ø¯ ØªØ§ ÛŒÚ© Ø±Ø¨Ø§Øª  (Agent) Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù‡Ø¯Ù Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin: 20px\" class=\"alert alert-block alert-success\"> <font size=\"5\">\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> <b>  Ù…Ø­ÛŒØ· Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø§Ø²ÛŒ: </b>  </font> </div>     \n",
    "<b> <center> <tt>\n",
    "--------------------------------<br />\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;-&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|<br />\n",
    "--------------------------------<br />\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;#&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;#&ensp;&ensp;|<br />\n",
    "--------------------------------<br />\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;$&ensp;&ensp;|<br />\n",
    "--------------------------------<br />\n",
    "&ensp;&ensp;&ensp;A&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;|<br />\n",
    "</tt> </center> </b> </font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">  \n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù…Ø­ÛŒØ· Ø±Ø¨Ø§Øª Ø¨Ø§ <code>A</code>ØŒ Ú¯Ù†Ø¬ Ø¨Ø§ <code>$</code>ØŒ Ú©Ù„ÛŒØ¯ Ø¨Ø§ <code>-</code> Ùˆ Ú¯ÙˆØ¯Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ú¯ Ø¨Ø§ <code>#</code> Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø§Ø²ÛŒ Ø¨Ù‡ Ø§ÛŒÙ† ØµÙˆØ±Øª Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ø±Ø¨Ø§Øª Ø¨Ù‡â€Œ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø¯Ø± ÛŒÚ©ÛŒ Ø§Ø² Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø²ÛŒ (Ø¨Ù‡ ØºÛŒØ± Ø§Ø² Ø®Ø§Ù†Ù‡â€ŒÛŒ Ú¯Ù†Ø¬ØŒ Ú©Ù„ÛŒØ¯ Ùˆ Ú¯ÙˆØ¯Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ú¯) Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯. Ø¯Ø± Ù‡Ø± Ù…Ø±Ø­Ù„Ù‡ Ø±Ø¨Ø§Øª Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ÛŒÚ©ÛŒ Ø§Ø² Actionâ€Œ Ù‡Ø§ÛŒ Ø±Ø§Ø³ØªØŒ Ø¨Ø§Ù„Ø§ØŒ Ú†Ù¾ Ùˆ Ù¾Ø§ÛŒÛŒÙ† Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯. Ø§Ø² Ø¢Ù†Ø¬Ø§ Ú©Ù‡ Ø§ÛŒÙ† Ù…Ø­ÛŒØ· Ù„ØºØ²Ù†Ø¯Ù‡ Ø§Ø³ØªØŒ Ø±Ø¨Ø§Øª Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ <code>p</code> Ø¯Ø± Ø¬Ù‡Øª Ø¯Ø±Ø³Øª Ùˆ Ø¨Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ <code>q = 1-p</code> Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ø¯Ø± Ø¬Ù‡Øª Û¹Û° Ø¯Ø±Ø¬Ù‡ Ù…Ø«Ù„Ø«Ø§ØªÛŒ Ø§Ú©Ø´Ù† Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ ÛŒÚ© Ú¯Ø§Ù… Ø­Ø±Ú©Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ø±Ø¨Ø§Øª Ø¨Ù‡ Ø¯ÛŒÙˆØ§Ø± Ø¨Ø§Ø²ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯ Ú©Ù†Ø¯ Ø¯Ø± Ù‡Ù…Ø§Ù† Ù…ÙˆÙ‚Ø¹ÛŒØª Ù‚Ø¨Ù„ÛŒ Ø®ÙˆØ¯ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯ Ùˆ Ø§Ú¯Ø± ÙˆØ§Ø±Ø¯ Ú¯ÙˆØ¯Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ú¯ Ø´ÙˆØ¯ Ø¨Ø§Ø²ÛŒ ØªÙ…Ø§Ù…ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ù‡Ù…ÛŒØ´Ù‡ Ø¯Ø± Ø¢Ù†Ø¬Ø§ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯. Ù‡Ø¯Ù Ø¨Ø§Ø²ÛŒ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø´Ù…Ø§ Ø§ÛŒÙ† Ø±Ø¨Ø§Øª Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ú©Ù‡ ØªØ§ Ø­Ø¯ Ù…Ù…Ú©Ù† Ø¯Ø± Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒÙ† Ù…Ø³ÛŒØ± Ú©Ù„ÛŒØ¯ Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±Ø¯ Ùˆ Ø¨Ù‡ Ø®Ø§Ù†Ù‡â€Œâ€ŒÛŒ Ú¯Ù†Ø¬ Ø¨Ø±ÙˆØ¯ Ùˆ Ø¨Ø§Ø²ÛŒ Ø±Ø§ Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø¨Ø±Ø³Ø§Ù†Ø¯. Ù¾Ø³ Ø§Ø² Ø¨Ø±Ø¯Ø§Ø´ØªÙ† Ú©Ù„ÛŒØ¯ØŒ Ø¹Ù„Ø§Ù…Øª <code>-</code> Ø¨Ù‡ <code>+</code>  ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯. State Ø¨Ø§Ø²ÛŒ Ø¨Ø§\n",
    "<div dir=\"ltr\"> <center> <b> ((x,y),z) </b> </center></div> <br />  \n",
    "Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ (x,y) Ù…ÙˆÙ‚Ø¹ÛŒØª Ø±Ø¨Ø§Øª Ùˆ z ÛŒÚ© Ø¹Ø¯Ø¯ Ø¨Ø§ÛŒÙ†Ø±ÛŒ Ø§Ø³Øª Ú©Ù‡ 0 Ø¨Ù‡ Ù…Ø¹Ù†ÛŒ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø±Ø¨Ø§Øª Ù‡Ù†ÙˆØ² Ú©Ù„ÛŒØ¯ Ø±Ø§ Ø¨Ø±Ù†Ø¯Ø§Ø´ØªÙ‡ Ùˆ 1 Ø¨Ù‡ Ù…Ø¹Ù†ÛŒ Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ú©Ù„ÛŒØ¯ Ø¨Ø±Ø¯Ø§Ø´ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</font> </div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> Ù…Ø­ÛŒØ· Ø§ÛŒÙ† Ø¨Ø§Ø²ÛŒ Ø¨Ù‡ Ú©Ù…Ú© Ú©Ù„Ø§Ø³ </font> </div>\n",
    "\n",
    "```python\n",
    "env = ENV(grid=(4,4), key=(0,2), coin=(2,3), holes=[(1,2),(1,3)], p=0.8, rewards='default')\n",
    "```\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ù…Ø´Ø®ØµØ§Øª Ù…Ø­ÛŒØ· Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø´Ø§Ù…Ù„ Ø³Ø§ÛŒØ² Ø´Ø¨Ú©Ù‡ Ø¨Ø§Ø²ÛŒØŒ Ù…ÙˆÙ‚Ø¹ÛŒØª Ú©Ù„ÛŒØ¯ØŒ Ù…ÙˆÙ‚Ø¹ÛŒØª Ú¯Ù†Ø¬ØŒ Ù…ÙˆÙ‚Ø¹ÛŒØª Ú¯ÙˆØ¯Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø±Ú¯ØŒ Ø§Ø­ØªÙ…Ø§Ù„ <code>p</code> Ùˆ <code>rewards</code> Ø¨Ø§Ø²ÛŒ Ø§Ø³Øª. Ù…ØªØºÛŒØ± <code>rewards</code> Ù…Ø´Ø®Øµ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù…Ø­ÛŒØ· Ú†Ù‡ Ù¾Ø§Ø¯Ø§Ø´ÛŒ Ø±Ø§ Ø¨Ù‡ Ø±Ø¨Ø§Øª Ø¨Ø§ ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù† Ø¨Ù‡ Ù‡Ø± Ø§Ø³ØªÛŒØª Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯ ØªÙ…Ø§Ù… Ø§Ø³ØªÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒØ§Ù†ÛŒ Ø¨Ø§ Ø§Ø­ØªÙ…Ø§Ù„ 1 Ùˆ Ù¾Ø§Ø¯Ø§Ø´ 0 Ø¨Ù‡ Ø®ÙˆØ¯Ø´Ø§Ù† Ø§Ù†ØªÙ‚Ø§Ù„ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. Ø¨Ù‡ Ø·ÙˆØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù¾Ø§Ø¯Ø§Ø´ Ù‡Ù…Ù‡â€ŒÛŒ Ø§Ø³ØªÛŒØªâ€ŒÙ‡Ø§ ØµÙØ± Ø§Ø³Øª Ù…Ú¯Ø± Ø§Ø³ØªÛŒØªÛŒ Ú©Ù‡ Ø±Ø¨Ø§Øª Ø¯Ø± Ù…ÙˆÙ‚Ø¹ÛŒØª Ú¯Ù†Ø¬ Ø¨Ø§Ø´Ø¯ Ùˆ Ú©Ù„ÛŒØ¯ Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ Ú©Ù‡ Ù¾Ø§Ø¯Ø§Ø´ 1 Ø¯Ø§Ø±Ø¯. Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù…ÛŒ Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§ÛŒ ØªÙ…Ø±ÛŒÙ† Ø§Ø² Ù…Ù‚Ø§Ø¯ÛŒØ± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø¨Ø§Ø²ÛŒ (Ù…Ø­ÛŒØ· Ø±Ø³Ù… Ø´Ø¯Ù‡ Ø¯Ø± Ø´Ú©Ù„ Ø¨Ø§Ù„Ø§ Ùˆ Ù¾Ø§Ø¯Ø§Ø´â€Œ Ù¾ÛŒØ´â€ŒÙØ±Ø¶) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ØŒ Ù…Ú¯Ø± Ø³ÙˆØ§Ù„ Ø§Ø² Ø´Ù…Ø§ Ø¨Ø®ÙˆØ§Ù‡Ø¯ Ø§ÛŒÙ† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯. Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø®Ø±ÙˆØ¬ÛŒ Ù…ØªØºÛŒØ± Ùˆ ØªÙˆØ§Ø¨Ø¹ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:    \n",
    "</font> </div>\n",
    "<br />\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>env.action_space</code> ÙØ¶Ø§ÛŒ Ø§Ú©Ø´Ù† Ù…Ø­ÛŒØ· </font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>env.state_space</code> ÙØ¶Ø§ÛŒ Ø§Ø³ØªÛŒØª Ù…Ø­ÛŒØ· Ùˆ Ù…Ø´Ø®Øµ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ† Ú©Ù‡ ÛŒÚ© Ø§Ø³ØªÛŒØª Ù†Ù‡Ø§ÛŒÛŒ Ù‡Ø³Øª ÛŒØ§ Ø®ÛŒØ± </font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>env.probability_space</code> ØªÙˆØ²ÛŒØ¹  p(sâ€™,r|s,a). Ø¨Ù‡ Ø§Ø²Ø§ÛŒ Ù‡Ø± (s,a) Ø§Ø­ØªÙ…Ø§Ù„ Ø±Ø®Ø¯Ø§Ø¯ (sâ€™,r) Ø±Ø§ Ø¨Ù‡ Ø´Ù…Ø§ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. <b>ÙÙ‚Ø· Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ model-based Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø§ÛŒÙ† Ù…ØªØºÛŒÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯</b>.\n",
    " </font> </div>\n",
    " \n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>()state = env.reset</code> Ø¨Ø§ Ø¨Ù‡ Ø·ÙˆØ± ØªØµØ§Ø¯ÙÛŒ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ† Ø±Ø¨Ø§Øª Ø¯Ø± ÛŒÚ©ÛŒ Ø§Ø² Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§Ø²ÛŒ Ø´Ø±ÙˆØ¹ Ù…ÛŒâ€ŒØ´ÙˆØ¯. </font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>state, reward, done = env.move(state, action)</code>  Ø¨Ø§ Ú¯Ø±ÙØªÙ† ÛŒÚ© Ø§Ø³ØªÛŒØª Ùˆ Ø§Ú©Ø´Ù† ÛŒÚ© Ø§Ù†ØªÙ‚Ø§Ù„ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø§Ø³ØªÛŒØª Ø¬Ø¯ÛŒØ¯ØŒ Ù¾Ø§Ø¯Ø§Ø´ Ùˆ Ø§ÛŒÙ† Ú©Ù‡ Ø§ÛŒÙ† Ø§Ø³ØªÛŒØª Ù†Ù‡Ø§ÛŒÛŒ Ù‡Ø³Øª ÛŒØ§ Ø®ÛŒØ± Ø±Ø§ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. <b> Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² ÙÙ‚Ø· Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ model-based Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    " </b></font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>state, reward, done = env.step(action)</code> ÛŒÚ© Ø§Ú©Ø´Ù† Ø¯Ø± Ø§Ø³ØªÛŒØª ÙØ¹Ù„ÛŒ Ø¨Ø§Ø²ÛŒ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. </font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>env.visual_screen(state)</code> ÛŒÚ© ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø§Ø³ØªÛŒØª Ø¨Ø§Ø²ÛŒ</font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"3\" face=\"HM XNiloofar\"> <code>env.visual_state_date(data)</code> ÛŒÚ© ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ value Ùˆ policy Ù‡Ù…Ù‡â€ŒÛŒ Ø§Ø³ØªÛŒØªâ€ŒÙ‡Ø§\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENV:\n",
    "    def __init__(self, grid=(4,4), key=(0,2), coin=(2,3), holes=[(1,2),(1,3)], p=0.8, rewards='default'):\n",
    "        \n",
    "        '''\n",
    "        env = ENV()\n",
    "        Allowed variables and functions in the model-based problems:\n",
    "            env.action_space | env.state_space | env.probability_space\n",
    "            env.reset() | env.move() | env.step()\n",
    " \n",
    "         Allowed variables and functions in the model-free problems:\n",
    "            env.action_space | env.state_space\n",
    "            env.reset() | env.step()\n",
    "            \n",
    "        Visualization functions are always allowed:\n",
    "            env.visual_screen() | env.visual_state_date()\n",
    "        '''\n",
    "        \n",
    "        self.grid = grid\n",
    "        self.key = key\n",
    "        self.coin = coin\n",
    "        self.holes = holes\n",
    "        \n",
    "        all_loc = set(list(itertools.product(range(grid[0]), range(grid[1]))))\n",
    "        all_disallowed_loc = set([key, coin] + holes)\n",
    "        self.all_allowed_loc = all_loc - all_disallowed_loc\n",
    "        \n",
    "        self.action_space = {'R':(0,1), 'U':(-1,0), 'L':(0,-1), 'D':(1,0)}\n",
    "        self.state_space = {state:(state[0] in holes) or (state == (coin,1)) for state in list(itertools.product(list(all_loc),[0,1]))}\n",
    "        rewards = {state: 1.0 if state == (coin,1) else 0.0 for state in self.state_space.keys()} if rewards == 'default' else rewards\n",
    "        self.probability_space = {}\n",
    "        \n",
    "        def move_straight(state, action):\n",
    "            if self.state_space[state] == True:\n",
    "                return state\n",
    "            else:\n",
    "                old_loc, old_key = state\n",
    "                desired_loc = tuple(map(sum, zip(old_loc, self.action_space[action])))\n",
    "                new_loc = desired_loc if desired_loc in all_loc else old_loc\n",
    "                new_key = 1 if new_loc == key else old_key\n",
    "                new_state = (new_loc, new_key)\n",
    "                return new_state\n",
    "        wrong = {'R':'U', 'U':'L', 'L':'D', 'D':'R'}\n",
    "        for state, action in list(itertools.product(self.state_space.keys(), self.action_space.keys())):\n",
    "            state_straight = move_straight(state, action)\n",
    "            state_wrong = move_straight(state, wrong[action])\n",
    "            reward_state_straight = rewards[state_straight] if not self.state_space[state] else 0.0\n",
    "            reward_state_wrong = rewards[state_wrong] if not self.state_space[state] else 0.0\n",
    "            self.probability_space[(state, action)] = {(state_straight, reward_state_straight):p, (state_wrong, reward_state_wrong):1.0-p} if not(state_straight==state_wrong) else {(state_straight, reward_state_straight):1.0}\n",
    "            \n",
    "        self.state = None    \n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        num = len(list(self.all_allowed_loc))\n",
    "        self.state = (list(self.all_allowed_loc)[np.random.choice(num)], 0)\n",
    "        return self.state\n",
    "    \n",
    "    def move(self, state, action):\n",
    "        num = len(self.probability_space[(state, action)].keys())\n",
    "        (next_state, reward) = list(self.probability_space[(state, action)].keys())[np.random.choice(num,p=list(self.probability_space[(state, action)].values()))]\n",
    "        return next_state, reward, self.state_space[next_state]\n",
    "    \n",
    "    def step(self, action):\n",
    "        self.state, reward, done = self.move(self.state, action)\n",
    "        return self.state, reward, done\n",
    "    \n",
    "    def visual_screen(self, state):\n",
    "        for i in range(self.grid[0]):\n",
    "            print(\"--------\" * self.grid[1])\n",
    "            for j in range(self.grid[1]):\n",
    "                x = \"A\" if (i,j) == state[0] else \" \"\n",
    "                if (i,j) == self.key and state[1] == 0: y = \"-\"\n",
    "                elif (i,j) == self.key and state[1] == 1: y = \"+\"\n",
    "                elif (i,j) == self.coin: y = \"$\"\n",
    "                elif (i,j) in self.holes: y = \"#\"\n",
    "                else: y = \" \"\n",
    "                print(\"   {:.2}  |\".format(x+y), end=\"\")\n",
    "            print(\"\")\n",
    "\n",
    "    def visual_state_date(self, data):\n",
    "        strtype = data[((0,0),0)] in ['R','U','L','D']\n",
    "        for i in range(self.grid[0]):\n",
    "            print(2*(\"--------\" * self.grid[1] + \"        \"))\n",
    "            for k in range(2):\n",
    "                for j in range(self.grid[1]):\n",
    "                    x = data[((i,j),k)]\n",
    "                    if (i,j) == self.key and k == 0: y = \"-\"\n",
    "                    elif (i,j) == self.key and k == 1: y = \"+\"\n",
    "                    elif (i,j) == self.coin: y = \"$\"\n",
    "                    elif (i,j) in self.holes: y = \"#\"\n",
    "                    else: y = \" \"\n",
    "                    if strtype:\n",
    "                        print(\"   {}{}  |\".format(x,y), end=\"\")\n",
    "                    elif x >= 0:\n",
    "                        print(\" {:.2f} {}|\".format(x,y), end=\"\")\n",
    "                    else:\n",
    "                        print(\"{:.2f} {}|\".format(x,y), end=\"\")\n",
    "                print(\"        \".format(x,y), end=\"\")\n",
    "            print(\"\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. An experience with the environment\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "    Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø¨Ø§ Ù…Ø­ÛŒØ· Ø¨Ø§Ø²ÛŒ Ùˆ ØªÙˆØ§Ø¨Ø¹ Ø¢Ù† Ø¢Ø´Ù†Ø§ Ø´ÙˆÛŒÙ….\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Cell Ù‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø´Ù…Ø§ Ø¨Ø§ Ù…Ø­ÛŒØ· Ø¨Ø§Ø²ÛŒ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intro\n",
    "env = ENV(p=0.8)\n",
    "\n",
    "# env.state_space and env.action_space\n",
    "print('env.state_space =', env.state_space, '\\n')\n",
    "print('env.action_space =',env.action_space, '\\n')\n",
    "\n",
    "# set a random value\n",
    "value = {state:np.random.rand() for state in env.state_space}\n",
    "\n",
    "# set a random policy\n",
    "policy = {state:np.random.choice(list(env.action_space.keys())) for state in env.state_space}\n",
    "\n",
    "# visual value\n",
    "env.visual_state_date(value)\n",
    "print(\"\")\n",
    "\n",
    "# visual policy\n",
    "env.visual_state_date(policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL a policy in a game\n",
    "def EVAL(env, policy, max_step_per_episode=100):\n",
    "    states_track = []\n",
    "    rewards_track = []\n",
    "    \n",
    "    # game loop\n",
    "    state = env.reset(); states_track.append(state)\n",
    "    for num_step in range(max_step_per_episode):\n",
    "        state, reward, done = env.step(policy[state]); states_track.append(state); rewards_track.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return states_track, sum(rewards_track), num_step+1\n",
    "\n",
    "# visualize game by gif\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def gif(env, states_track, delay):\n",
    "    for state in states_track:\n",
    "        clear_output(wait=True)\n",
    "        env.visual_screen(state)\n",
    "        sleep(delay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL a random policy\n",
    "env = ENV(p=0.8)\n",
    "states_track, reward_sum, num_step = EVAL(env, policy)\n",
    "print(reward_sum, num_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif\n",
    "gif(env, states_track, 10/num_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Code\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "    Ú©Ø¯ Ú©Ù„Ø§Ø³ <code>ENV</code> Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø®ØªØµØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ Ú†Ú¯ÙˆÙ†Ú¯ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¢Ù† ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Policy\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "   Cell Ø²ÛŒØ± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ù† Ø±Ø§ Ø¢Ù†Ø§Ù„ÛŒØ² Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL the random policy\n",
    "env = ENV(p=0.8)\n",
    "\n",
    "num_test = 10000\n",
    "total_reward = 0.0\n",
    "for t in range(num_test):\n",
    "    policy = {state:np.random.choice(list(env.action_space.keys())) for state in env.state_space}\n",
    "    states_track, reward_sum, num_step = EVAL(env, policy); total_reward += reward_sum\n",
    "print(total_reward / num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Policy Iteration\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Policy Iteration Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒÙ…. Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ø§Ø³Øª:\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The used Bellman equations:**\n",
    "\n",
    "Policy evaluation (Recursive for $V$ in $\\pi$):\n",
    "\n",
    "$V_{\\pi}(s) = \\sum_{a \\in A} \\pi(a|s) \\Bigg(\\sum_{s' \\in S} p(s'|s,a) \\Big(r(s,a,s') + \\gamma V_{\\pi}(s') \\Big) \\Bigg)$\n",
    "\n",
    "Policy improvement (For $V$ in $\\pi_{*}$):\n",
    "\n",
    "$\\pi_{*}(s) = \\arg\\max_a \\sum_{s' \\in S} p(s'|s,a) \\Big(r(s,a,s') + \\gamma V_{*}(s') \\Big)$ \n",
    "\n",
    "Note ([Wikipedia]): For all final states $s_{f}$, $V(s_{f})$ and $Q(s_{f},a)$ are never updated, but is set to the reward value $r$ observed for state $s_{f}$. In most cases, $V(s)$ and $Q(s_{f},a)$ can be taken to equal zero.\n",
    "\n",
    "[Wikipedia]: https://en.wikipedia.org/wiki/Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial value of final states\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Ú†Ø±Ø§ Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ø±Ø§Ø³Ø§Ø³ bootstrapping Ø¨Ø§ÛŒØ¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ <code>value</code> Ø¯Ø± Ø§Ø³ØªÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§Ø¨Ø± ØµÙØ± ØªÙ†Ø¸ÛŒÙ… Ú¯Ø±Ø¯Ø¯ØŸ\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Guess the optimal policy\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">Ù‚Ø¨Ù„ Ø§Ø² Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…ØŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù†ÛŒØ¯ Ú©Ù‡ <code>policy</code> Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ <code>p=[0.2, 0.6, 0.8, 1.0]</code> Ú†Ú¯ÙˆÙ†Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯ØŸ Ù†ÛŒØ§Ø² Ù†ÛŒØ³Øª Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ Ø¯Ù‚ÛŒÙ‚ Ùˆ Ù…Ù†Ø·Ø¨Ù‚ Ø¨Ø± Ù†ØªØ§ÛŒØ¬ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§Ø´Ø¯ ÙˆÙ„ÛŒ Ù…Ø³ØªØ¯Ù„ Ø¢Ù† Ø±Ø§ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Policy Iteration Algorithm\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Policy Iteration Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯:\n",
    "</font> </div>\n",
    "\n",
    "\n",
    "```python\n",
    "value, policy, value_track, policy_track, num_iter = Train_PolicyIteration(env, GAMMA=0.9, SMALL_ENOUGH=1e-3)\n",
    "```\n",
    "\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø­ÛŒØ· Ø¨Ø§Ø²ÛŒØŒ Ú¯Ø§Ù…Ø§ Ùˆ Ø§Ø³ØªØ§Ù†Ù‡ ØªÙˆÙ‚Ù Policy Evaluation Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ <code>value</code> Ø¨Ù‡ÛŒÙ†Ù‡ØŒ <code>policy</code> Ø¨Ù‡ÛŒÙ†Ù‡ØŒ Ù„ÛŒØ³Øª ØªØ§Ø±ÛŒØ®Ú†Ù‡ <code>value</code>  Ø¨Ù‡ Ø±ÙˆØ² Ø´Ø¯Ù‡ Ø¨Ø¹Ø¯ Ù‡Ø± Ø¨Ø§Ø± Ø§Ø¬Ø±Ø§ÛŒ Policy EvaluationØŒ Ù„ÛŒØ³Øª ØªØ§Ø±ÛŒØ®Ú†Ù‡ <code>policy</code>  Ø¨Ù‡ Ø±ÙˆØ² Ø´Ø¯Ù‡ Ø¨Ø¹Ø¯ Ù‡Ø± Ø¨Ø§Ø± Ø§Ø¬Ø±Ø§ÛŒ Policy Improvement Ùˆ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø±Ù‡Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ (Ù‡Ø± Ø¨Ù‡ Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø± <code>value</code> ÛŒØ§ <code>policy</code> ÛŒÚ© Ø§Ø³ØªÛŒØª ÛŒÚ© ØªÚ©Ø±Ø§Ø± Ø§Ø³Øª)  Ø±Ø§ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    " </font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Iteration Alg.\n",
    "\n",
    "def Train_PolicyIteration(env, GAMMA=0.9, SMALL_ENOUGH=1e-3):\n",
    "    value_track = []\n",
    "    policy_track = []\n",
    "    num_iter = 0\n",
    "\n",
    "    # Initialization --------------------------------------------------\n",
    "    # set a random value\n",
    "    value = None # TODO\n",
    "    # set a random policy\n",
    "    policy = None # TODO\n",
    "    # track\n",
    "    value_track.append(value.copy()); policy_track.append(policy.copy())\n",
    "\n",
    "    while True:\n",
    "        # Policy Evaluation --------------------------------------------------\n",
    "        # TODO\n",
    "        \n",
    "        # Policy Improvement --------------------------------------------------\n",
    "        # TODO\n",
    "\n",
    "        # track\n",
    "        value_track.append(value.copy()); policy_track.append(policy.copy())\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "    return value, policy, value_track, policy_track, num_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze the results\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Cell Ø²ÛŒØ± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ù‡ Ø·ÙˆØ± Ù…ÙØµÙ„ Ù†ØªØ§ÛŒØ¬ Ø¢Ù† (Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒØŒ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø±ØŒ <code>value</code>  Ø¨Ù‡ÛŒÙ†Ù‡ Ùˆ <code>policy</code> Ø¨Ù‡ÛŒÙ†Ù‡ Ø¯Ø± Ù‡Ø± <code>p</code>) Ø±Ø§ ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> Ø¨Ù‡ Ø·ÙˆØ± Ø®Ø§Øµ\n",
    "Ø¨Ù‡ Ø§Ø²Ø§ÛŒ <code>p</code> Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„ÙØŒ Ù†ØªØ§ÛŒØ¬ <code>policy</code> Ø¨Ù‡ÛŒÙ†Ù‡ Ø¯Ø± Ø§Ø³ØªÛŒØªâ€ŒÙ‡Ø§ÛŒ    Ø²ÛŒØ± Ø±Ø§ Ø¢Ù†Ø§Ù„ÛŒØ² Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>  \n",
    "\n",
    "```python\n",
    "States: ((0,2),0), ((0,2),1), ((1,1),1)\n",
    "```\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Ø¨Ù‡ ØµÙˆØ±Øª Ø¯Ø³ØªÛŒ Ø¨Ø±Ø§ÛŒ <code>policy</code> Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¯Ø± <code>p</code> Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„ÙØŒ Ù…Ù‚Ø¯Ø§Ø± <code>value</code> Ø¯Ø± Ø§Ø³ØªÛŒØªâ€ŒÙ‡Ø§ÛŒ  Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ Ø¯Ø³Øª Ø¢ÙˆØ±ÛŒØ¯ Ùˆ Ø¨Ø§ Ù†ØªÛŒØ¬Ù‡â€ŒÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>\n",
    "\n",
    "```python\n",
    "States: ((3,3),1), ((2,2),1), ((3,2),1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Alg.\n",
    "p_list = [1.0, 0.8, 0.6, 0.2]\n",
    "GAMMA = 0.9; SMALL_ENOUGH = 1e-3\n",
    "\n",
    "\n",
    "num_test = 100\n",
    "for p in p_list:\n",
    "    \n",
    "    # for a probability p\n",
    "    env = ENV(p=p)\n",
    "    opt_value_track = []\n",
    "    number_of_iteration = 0\n",
    "    for t in range(num_test):\n",
    "        value, policy, value_track, policy_track, num_iter = Train_PolicyIteration(env, GAMMA, SMALL_ENOUGH)\n",
    "        opt_value_track.append(value)\n",
    "        number_of_iteration += num_iter\n",
    "    \n",
    "    # visual\n",
    "    print(f\"p = {p} \"+90*'~')\n",
    "    print(f\"convergence = {all(((np.array(list(element.values())) - np.array(list(opt_value_track[0].values())))**2).mean() <= 1e-4 for element in opt_value_track)} | number of iteration = {number_of_iteration / num_test}\")\n",
    "    env.visual_state_date(value)\n",
    "    env.visual_state_date(policy)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot $V_{\\pi_k}(s)$ vs $k$\n",
    "\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø­ÛŒØ· Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ± <code>p=0.8</code> Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯. Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø¯Ø§Ø± <code>value</code> Ø§Ø³ØªÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø± Initialization Ùˆ Ø¨Ø¹Ø¯ Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ø§Ø± Policy Evaluation Ø±Ø³Ù… Ú©Ù†ÛŒØ¯. Ù‡Ù…Ù‡â€ŒÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± ÛŒÚ© plot Ø±Ø³Ù… Ú©Ù†ÛŒØ¯. Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¢Ù†Ø§Ù„ÛŒØ² Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>\n",
    "\n",
    "```python\n",
    "States: ((3,0),0), ((2,1),0), ((0,2),1), ((1,1),1), ((2,2),1)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Cell Ø²ÛŒØ± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø´Ù…Ø§ Ø±Ø§ Ø¯Ø± ÛŒÚ© Ø¨Ø§Ø²ÛŒ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif\n",
    "GAMMA = 0.9; SMALL_ENOUGH = 1e-3\n",
    "\n",
    "env = ENV(p=0.8)\n",
    "_, policy, _, _, _= Train_PolicyIteration(env, GAMMA, SMALL_ENOUGH)\n",
    "states_track, reward_sum, num_step = EVAL(env, policy)\n",
    "\n",
    "gif(env, states_track, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Effects of the discount factor $\\gamma$\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Cell Ø²ÛŒØ± ØªØ§Ø«ÛŒØ± Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ù‚Ø¯Ø§Ø± <code>GAMMA</code> Ø¨Ø± <code>policy</code> Ø¨Ù‡ÛŒÙ†Ù‡ Ø¯Ø± Ù…Ø­ÛŒØ·â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¢Ù†Ø§Ù„ÛŒØ² Ú©Ù†ÛŒØ¯.\n",
    "    <br />\n",
    "Ú†Ø±Ø§ Ù†ØªØ§ÛŒØ¬ ØªÙ‚Ø§Ø±Ù† Ù†Ø¯Ø§Ø±Ø¯Ø› ÛŒØ¹Ù†ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Agent Ø¯Ø± <code>ENV(p=p)</code> Ùˆ <code>ENV(p=1-p)</code> ÛŒÚ©Ø³Ø§Ù† Ù†ÛŒØ³Øª.\n",
    "     <br />\n",
    " <code>cumulative reward</code>  ÛŒØ§ Ù¾Ø§Ø¯Ø§Ø´ ØªØ¬Ù…Ø¹ÛŒ Ø¯Ø± ÛŒÚ© episode Ø¨Ù‡ Ù…Ø¹Ù†ÛŒ Ø¬Ù…Ø¹ ØªÙ…Ø§Ù… Ù¾Ø§Ø¯Ø§Ø´â€ŒÙ‡Ø§ Ø¨Ø¯ÙˆÙ† Ø§Ø­ØªØ³Ø§Ø¨ Ú¯Ø§Ù…Ø§ Ø§Ø³Øª.   \n",
    " </font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL the optimal policy\n",
    "p_list2 = np.linspace(0.,1.,50)\n",
    "GAMMA_list = [0.2, 0.5, 0.9, 0.99]; SMALL_ENOUGH = 1e-3\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "for GAMMA in GAMMA_list:\n",
    "    r_list = []\n",
    "    n_list = []\n",
    "    for p in p_list2:\n",
    "        env = ENV(p=p)\n",
    "        _, policy, _, _, _= Train_PolicyIteration(env, GAMMA, SMALL_ENOUGH)\n",
    "\n",
    "        num_test = 1000\n",
    "        total_reward = 0.0\n",
    "        total_num_step = 0\n",
    "        for t in range(num_test):\n",
    "            states_track, reward_sum, num_step = EVAL(env, policy); total_reward += reward_sum; total_num_step += num_step;\n",
    "        r_list.append(total_reward / num_test)\n",
    "        n_list.append(total_num_step / num_test)\n",
    "\n",
    "    ax[0].plot(p_list2, r_list, label=f\"GAMMA = {GAMMA}\")\n",
    "    ax[1].plot(p_list2, n_list, label=f\"GAMMA = {GAMMA}\")\n",
    "\n",
    "ax[0].set_xlabel('p')\n",
    "ax[0].set_ylabel('the average cumulative reward per episode')\n",
    "ax[1].set_xlabel('p')\n",
    "ax[1].set_ylabel('the average number of steps per episode')\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Case $\\gamma = 1$\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Ú©Ø§Ø±Ú©Ø±Ø¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ <code>GAMMA = 1</code> Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯. Ø§Ú¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø´Ù…Ø§ Ù‡Ù…Ú¯Ø±Ø§ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¯Ù„ÛŒÙ„ Ø¢Ù† Ø±Ø§ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.\n",
    " </font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the true value to compare with other algorithms\n",
    "p_list = [1.0, 0.8, 0.6, 0.2]\n",
    "GAMMA = 0.9; SMALL_ENOUGH = 1e-3\n",
    "\n",
    "save_value = {p: Train_PolicyIteration(ENV(p=p), GAMMA, SMALL_ENOUGH)[0] for p in p_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Value Iteration\n",
    "\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Value Iteration Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒÙ…. Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ø§Ø³Øª:\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The used Bellman equation:**\n",
    "\n",
    "Recursive for $V$ in $\\pi_{*}$:\n",
    "\n",
    "$V_{*}(s) = \\max_a \\sum_{s' \\in S} p(s'|s,a) \\Big(r(s,a,s') + \\gamma V_{*}(s') \\Big)$\n",
    "\n",
    "Note ([Wikipedia]): For all final states $s_{f}$, $V(s_{f})$ and $Q(s_{f},a)$ are never updated, but is set to the reward value $r$ observed for state $s_{f}$. In most cases, $V(s)$ and $Q(s_{f},a)$ can be taken to equal zero.\n",
    "\n",
    "[Wikipedia]: https://en.wikipedia.org/wiki/Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement Value Iteration Algorithm\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Value Iteration Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯:\n",
    "</font> </div>\n",
    "\n",
    "```python\n",
    "value, policy, value_track, num_iter = Train_ValueIteration(env, GAMMA=0.9, SMALL_ENOUGH=1e-3)\n",
    "```\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø­ÛŒØ· Ø¨Ø§Ø²ÛŒØŒ Ú¯Ø§Ù…Ø§ Ùˆ Ø§Ø³ØªØ§Ù†Ù‡ ØªÙˆÙ‚Ù Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ <code>value</code>  Ø¨Ù‡ÛŒÙ†Ù‡ØŒ <code>policy</code> Ø¨Ù‡ÛŒÙ†Ù‡ØŒ Ù„ÛŒØ³Øª ØªØ§Ø±ÛŒØ®Ú†Ù‡ <code>value</code>  Ø¨Ù‡ Ø±ÙˆØ² Ø´Ø¯Ù‡ Ø¨Ø¹Ø¯ Ù‡Ø± Ø¨Ø§Ø± loop  Ø¨Ø± Ø±ÙˆÛŒ ØªÙ…Ø§Ù… Ø§Ø³ØªÛŒØªâ€ŒÙ‡Ø§ Ùˆ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø±Ù‡Ø§ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ (Ù‡Ø± Ø¨Ù‡ Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø± <code>value</code> ÛŒÚ© Ø§Ø³ØªÛŒØª ÛŒÚ© ØªÚ©Ø±Ø§Ø± Ø§Ø³Øª) Ø±Ø§ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Alg.\n",
    "\n",
    "def Train_ValueIteration(env, GAMMA=0.9, SMALL_ENOUGH=1e-3):\n",
    "    value_track = []\n",
    "    num_iter = 0\n",
    "\n",
    "    # Initialization --------------------------------------------------\n",
    "    # set a random value\n",
    "    value = None # TODO\n",
    "    # track\n",
    "    value_track.append(value.copy())\n",
    "\n",
    "    # Value Iteration --------------------------------------------------\n",
    "    # TODO\n",
    "\n",
    "    # Compute a deterministic optimal policy ---------------------------\n",
    "    # TODO\n",
    "\n",
    "    return value, policy, value_track, num_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze the results\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Cell Ø²ÛŒØ± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø¢Ù† (Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒØŒ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø±ØŒ <code>value</code>  Ø¨Ù‡ÛŒÙ†Ù‡ Ùˆ <code>policy</code> Ø¨Ù‡ÛŒÙ†Ù‡ Ø¯Ø± Ù‡Ø± <code>p</code>) Ø±Ø§ Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Policy Iteration Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Alg.\n",
    "p_list = [1.0, 0.8, 0.6, 0.2]\n",
    "GAMMA = 0.9; SMALL_ENOUGH = 1e-3\n",
    "\n",
    "\n",
    "num_test = 100\n",
    "for p in p_list:\n",
    "    \n",
    "    # for a probability p\n",
    "    env = ENV(p=p)\n",
    "    opt_value_track = []\n",
    "    number_of_iteration = 0\n",
    "    for t in range(num_test):\n",
    "        value, policy, value_track, num_iter = Train_ValueIteration(env, GAMMA, SMALL_ENOUGH)\n",
    "        opt_value_track.append(value)\n",
    "        number_of_iteration += num_iter\n",
    "    \n",
    "    # visual\n",
    "    print(f\"p = {p} \"+90*'~')\n",
    "    print(f\"convergence = {all(((np.array(list(element.values())) - np.array(list(save_value[p].values())))**2).mean() <= 1e-4 for element in opt_value_track)} | number of iteration = {number_of_iteration / num_test}\")\n",
    "    env.visual_state_date(value)\n",
    "    env.visual_state_date(policy)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot $V_{\\pi_k}(s)$ vs $k$\n",
    "\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø­ÛŒØ· Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ± <code>p=0.8</code> Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯. Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø¯Ø§Ø± <code>value</code> Ø§Ø³ØªÛŒØªâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø± Initialization Ùˆ Ø¨Ø¹Ø¯ Ø§Ø¬Ø±Ø§ÛŒ Ù‡Ø± Ø¨Ø§Ø± loop Ø¨Ø± Ø±ÙˆÛŒ ØªÙ…Ø§Ù… Ø§Ø³ØªÛŒØªâ€ŒÙ‡Ø§ Ø±Ø³Ù… Ú©Ù†ÛŒØ¯. Ù‡Ù…Ù‡â€ŒÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± ÛŒÚ© plot Ø±Ø³Ù… Ú©Ù†ÛŒØ¯. Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¢Ù†Ø§Ù„ÛŒØ² Ùˆ Ø¨Ø§ Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ø´Ø§Ø¨Ù‡ Ø¯Ø± Ù‚Ø³Ù…Øª Policy Iteration Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>\n",
    "\n",
    "```python\n",
    "States: ((3,0),0), ((2,1),0), ((0,2),1), ((1,1),1), ((2,2),1)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D. Q-Learning\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Q-Learning Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒÙ…. Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ù‡ ØµÙˆØ±Øª Ø²ÛŒØ± Ø§Ø³Øª:\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The used Bellman equation:**\n",
    "\n",
    "Recursive for $Q$ in $\\pi_{*}$:\n",
    "\n",
    "$Q_{*}(s,a) \\overset{model-based}{=} \\sum_{s' \\in S} p(s'|s,a) \\Big(r(s,a,s') + \\gamma \\max_{a'}Q_*(s',a') \\Big)  \\overset{model-free}{=} \\mathbb{E} [R + \\gamma \\max_{a'}Q_*(S',a')]$\n",
    "\n",
    "\n",
    "The agent doesn't know $p(s'|s,a)$ and $r(s',s,a)$.\n",
    "\n",
    "[Wikipedia]: The agent receives $s_t$, then chooses an action $a_{t}$. The environment moves to a new state $s_{t+1}$ and the reward $r_{t+1}$ associated with the transition $(s_{t},a_{t},s_{t+1})$ is determined. The goal of reinforcement learning is to learn a policy: $\\pi :A\\times S\\rightarrow [0,1]$, $\\pi (a,s)=\\Pr(a_{t}=a\\mid s_{t}=s)$ which maximizes the expected cumulative reward. The core of the algorithm is a Bellman equation as a simple value iteration update, using the weighted average of the old value and the new information:\n",
    "\n",
    "[Wikipedia]: https://en.wikipedia.org/wiki/Q-learning\n",
    "\n",
    "\n",
    "$ Q^{new}(s_{t},a_{t})\\leftarrow \\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}+\\underbrace {\\alpha } _{\\text{learning rate}}\\cdot \\overbrace {{\\bigg (}\\underbrace {\\underbrace {r_{t+1}} _{\\text{reward}}+\\underbrace {\\gamma } _{\\text{discount factor}}\\cdot \\underbrace {\\max _{a}Q(s_{t+1},a)} _{\\text{estimate of optimal future value}}} _{\\text{new value (temporal difference target)}}-\\underbrace {Q(s_{t},a_{t})} _{\\text{old value}}{\\bigg )}} ^{\\text{temporal difference}}$\n",
    "\n",
    "How to update $Q(s,a)$?\n",
    "\n",
    "**1) Exploration and then Exploitation:**\n",
    "\n",
    "**By Loop:**\n",
    "```python\n",
    "init Q \n",
    "# For all final states s_f in final_states, Q(s_f,a) is never updated, but is set to the reward value r observed for state s_f. \n",
    "# In most cases, Q(s_f,a) can be taken to equal zero. \n",
    "\n",
    "while doesnt converge:\n",
    "  for s in S:\n",
    "    for a in A:\n",
    "      # the environment reaction\n",
    "      s_new, r = env.move(s,a)\n",
    "      # update Q(s,a)\n",
    "      Q(s,a) += alpha * (r + gamma * max(Q(s_new,:)) - Q(s,a))\n",
    "```\n",
    "\n",
    "\n",
    "**2) Exploration and Exploitation, at the same time:**\n",
    "\n",
    "**Play a game!**\n",
    "```python\n",
    "init Q \n",
    "# For all final states s_f in final_states, Q(s_f,a) is never updated, but is set to the reward value r observed for state s_f. \n",
    "# In most cases, Q(s_f,a) can be taken to equal zero. \n",
    "\n",
    "while doesnt converge:\n",
    "  # play a game!\n",
    "  s = random start state\n",
    "  # a <- policy(Q(s,:)) can be random policy or epsilon-greedy or softmax, etc.\n",
    "  while not s in final_states:\n",
    "    # the agent action in state s\n",
    "    a = policy(Q(s,:))\n",
    "    # the environment reaction\n",
    "    s_new, r = env.step(a)\n",
    "    # update Q(s,a)\n",
    "    Q(s,a) += alpha * (r + gamma * max(Q(s_new,:)) - Q(s,a))\n",
    "    # update state\n",
    "    s = s_new\n",
    "```\n",
    "\n",
    "Indeed, `policy(Q(s,:))` in Q-Learning is *the behavior policy*, not the target policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement Q-Learning Algorithm\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Q-Learning Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§ÙˆØ´ Ùˆ Ø¨Ù‡Ø±Ù‡â€ŒØ¨Ø±Ø¯Ø§Ø±ÛŒ Ù‡Ù…Ø²Ù…Ø§Ù† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯:\n",
    "</font> </div>\n",
    "\n",
    "```python\n",
    "value, policy, reward_episode_track, i_step_episode_track, final_state_track = Train_QLearning(env, GAMMA=0.9, ALPHA=0.1, max_step_per_episode=100, eps_max=1.0, eps_min=0.0, eps_num_explore=1000, eps_num_explore_exploit=8000, max_num_episode=1000)\n",
    "```\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\">\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ Ù…Ø­ÛŒØ· Ø¨Ø§Ø²ÛŒØŒ Ú¯Ø§Ù…Ø§ØŒ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒØŒ Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ step Ø¨Ø±Ø§ÛŒ Ù‡Ø±episodeØŒ Ø­Ø¯Ø§Ú©Ø«Ø± Ù…Ù‚Ø¯Ø§Ø± epsilonØŒ Ø­Ø¯Ø§Ù‚Ù„ Ù…Ù‚Ø¯Ø§Ø± epsilonØŒ ØªØ¹Ø¯Ø§Ø¯ step Ø§ÛŒ Ú©Ù‡ epsilon Ø¨Ø± Ø±ÙˆÛŒ Ù…Ù‚Ø¯Ø§Ø± Ø­Ø¯Ø§Ú©Ø«Ø± Ø®ÙˆØ¯ Ø§Ø³ØªØŒ ØªØ¹Ø¯Ø§Ø¯ step Ø§ÛŒ Ú©Ù‡ epsilon Ø¨Ù‡ ØµÙˆØ±Øª Ø®Ø·ÛŒ Ø§Ø² Ù…Ù‚Ø¯Ø§Ø± Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ø¨Ù‡ Ú©Ù…ÛŒÙ†Ù‡ Ù…ÛŒâ€ŒØ±Ø³Ø¯ Ùˆ ØªØ¹Ø¯Ø§Ø¯ episode Ø§ÛŒ Ú©Ù‡ Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ <code>value</code>  Ø¨Ù‡ÛŒÙ†Ù‡ØŒ <code>policy</code> Ø¨Ù‡ÛŒÙ†Ù‡ØŒ Ù„ÛŒØ³Øª reward ØªØ¬Ù…Ø¹ÛŒ Ù‡Ø± episodeØŒ Ù„ÛŒØ³Øª Ø´Ù…Ø§Ø±Ù‡â€ŒÛŒ step Ø§ÛŒ Ú©Ù‡ Ù‡Ø± episode Ø¯Ø± Ø¢Ù† Ù¾Ø§ÛŒØ§Ù† ÛŒØ§ÙØªÙ‡ Ùˆ Ù„ÛŒØ³Øª Ø§Ø³ØªÛŒØªÛŒ Ú©Ù‡ Ù‡Ø± episode Ø¨Ø§ Ø¢Ù† Ù¾Ø§ÛŒØ§Ù† ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª Ø±Ø§ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q Learning Alg.\n",
    "\n",
    "def Train_QLearning(env, GAMMA=0.9, ALPHA=0.1, max_step_per_episode=100, eps_max=1.0, eps_min=0.0, eps_num_explore=1000, eps_num_explore_exploit=8000, max_num_episode=1000):\n",
    "    reward_episode_track = []\n",
    "    i_step = 0\n",
    "    i_step_episode_track = []\n",
    "    final_state_track = []\n",
    "\n",
    "    # Initialization --------------------------------------------------\n",
    "    # set a random Q \n",
    "    Q = None # TODO\n",
    "\n",
    "    for _ in range(max_num_episode):\n",
    "        # Play a game! --------------------------------------------------\n",
    "        state = env.reset()\n",
    "        reward_episode_sum = 0.0\n",
    "        for num_step in range(max_step_per_episode):\n",
    "            i_step += 1\n",
    "            \n",
    "            # ---------------------- choose action ----------------------\n",
    "            # update epsilon\n",
    "            # TODO\n",
    "            # epsilon greedy\n",
    "            # TODO\n",
    "            \n",
    "            # ---------------------- do the step ----------------------\n",
    "            # TODO\n",
    "            reward_episode_sum += reward\n",
    "            \n",
    "            # ------------------------ update Q ------------------------\n",
    "            # TODO\n",
    "            \n",
    "            # -------------------- go to next_state --------------------\n",
    "            # TODO\n",
    "            \n",
    "            # ------------------------- if done -------------------------\n",
    "            if done:\n",
    "                break\n",
    "        reward_episode_track.append(reward_episode_sum)\n",
    "        i_step_episode_track.append(i_step + 1)\n",
    "        final_state_track.append(state)\n",
    "    \n",
    "    # Compute optimal values and a deterministic optimal policy --------------------------------------------------\n",
    "    # TODO\n",
    "    \n",
    "    return value, policy, reward_episode_track, i_step_episode_track, final_state_track\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze the results\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> Cell\n",
    " Ø²ÛŒØ± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ù‡ Ø·ÙˆØ± Ù…ÙØµÙ„ Ù†ØªØ§ÛŒØ¬ Ø¢Ù† (Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒØŒ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø±ØŒ reward  ØªØ¬Ù…Ø¹ÛŒ Ù‡Ø± episodeØŒ ØªØ¹Ø¯Ø§Ø¯ step Ù‡Ø± episodeØŒ Ù…Ù‚Ø¯Ø§Ø± <code>value</code>  Ø¨Ù‡ÛŒÙ†Ù‡ Ùˆ <code>policy</code> Ø¨Ù‡ÛŒÙ†Ù‡ Ø¯Ø± Ù‡Ø± <code>p</code>) Ø±Ø§ ØªØ¬Ø²ÛŒÙ‡ Ùˆ ØªØ­Ù„ÛŒÙ„ Ùˆ Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Eval Alg.\n",
    "p_list = [1.0, 0.8, 0.6, 0.2]\n",
    "GAMMA=0.9; ALPHA=0.1; max_step_per_episode=100; eps_max=1.0; eps_min=0.0; eps_num_explore=1000; eps_num_explore_exploit=8000; max_num_episode=1000\n",
    "\n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w\n",
    "\n",
    "num_test = 20\n",
    "for p in p_list:\n",
    "    \n",
    "    # for a probability p\n",
    "    env = ENV(p=p)\n",
    "    opt_value_track = []\n",
    "    reward_episode_track_sum = np.zeros(max_num_episode)\n",
    "    num_step_episode_track_sum = np.zeros(max_num_episode)\n",
    "    number_of_iteration = 0\n",
    "    for t in range(num_test):\n",
    "        value, policy, reward_episode_track, i_step_episode_track, final_state_track = Train_QLearning(env, GAMMA, ALPHA, max_step_per_episode, eps_max, eps_min, eps_num_explore, eps_num_explore_exploit, max_num_episode)\n",
    "        opt_value_track.append(value)\n",
    "        reward_episode_track_sum += np.array(reward_episode_track)\n",
    "        num_step_episode_track_sum += np.array(i_step_episode_track) - np.array([0]+i_step_episode_track[:-1])\n",
    "        number_of_iteration += i_step_episode_track[-1]\n",
    "    \n",
    "    # visual\n",
    "    print(f\"p = {p} \"+90*'~')\n",
    "    MSE = sum(((np.array(list(element.values())) - np.array(list(save_value[p].values())))**2).mean() for element in opt_value_track)/len(opt_value_track)\n",
    "    print(\"MSE convergence = {:.5f} | number of iteration = {}\".format(MSE,number_of_iteration/len(opt_value_track)))\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n",
    "    ax[0].plot(range(max_num_episode-9), moving_average(reward_episode_track_sum/num_test, 10), label=f\"p = {p}\")\n",
    "    ax[1].plot(range(max_num_episode-9), moving_average(num_step_episode_track_sum/num_test, 10), label=f\"p = {p}\")\n",
    "    ax[0].set_xlabel('episode')\n",
    "    ax[0].set_ylabel('the average cumulative reward per episode')\n",
    "    ax[1].set_xlabel('episode')\n",
    "    ax[1].set_ylabel('the average number of steps per episode')\n",
    "    ax[0].legend()\n",
    "    ax[1].legend()\n",
    "    plt.show()\n",
    "    env.visual_state_date(value)\n",
    "    env.visual_state_date(policy)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Cell Ø²ÛŒØ± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø´Ù…Ø§ Ø±Ø§ Ø¯Ø± ÛŒÚ© Ø¨Ø§Ø²ÛŒ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gif\n",
    "GAMMA=0.9; ALPHA=0.1; max_step_per_episode=100; eps_max=1.0; eps_min=0.0; eps_num_explore=1000; eps_num_explore_exploit=8000; max_num_episode=1000\n",
    "\n",
    "env = ENV(p=0.8)\n",
    "value, policy, _, _, _ = Train_QLearning(env, GAMMA, ALPHA, max_step_per_episode, eps_max, eps_min, eps_num_explore, eps_num_explore_exploit, max_num_episode)\n",
    "states_track, reward_sum, num_step = EVAL(env, policy)\n",
    "\n",
    "gif(env, states_track, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Effects of $\\alpha$ and $\\varepsilon$\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "    ØªØ§Ø«ÛŒØ± Ù¾Ø§Ø±Ø§Ù…ØªØ±â€ŒÙ‡Ø§ÛŒ <code>ALPHA</code> Ùˆ epsilon Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø±Ø§ Ø¨Ø± Ú©Ø§Ø±Ú©Ø±Ø¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using random policy\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "Ø¯Ø± Ú©Ù„Ø§Ø³ Ø¯Ø±Ø³ Ø´Ù…Ø§ Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Q-Learning Ø¨Ù‡ ØµÙˆØ±ØªÛŒ Ø¢Ø´Ù†Ø§ Ø´Ø¯ÛŒØ¯ Ú©Ù‡  Agent Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø§Ú©Ø´Ù† Ø¯Ø± Ù‡Ø± Ú¯Ø§Ù… Ø§Ø² epsilon-greedy-policy Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ù…Ø§ Ø¨Ù‡ Ø¬Ø§ÛŒ epsilon-greedy-policy Ø§Ø² random-policy (ÛŒØ¹Ù†ÛŒ Ø¯Ø± Ù‡Ø± Ú¯Ø§Ù… Ú©Ø§Ù…Ù„Ø§ ØªØµØ§Ø¯ÙÛŒ Ùˆ Ø¨Ø¯ÙˆÙ† ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± value ÛŒÚ© Ø§Ú©Ø´Ù† Ø§Ù†ØªØ®Ø§Ø¨ Ø´ÙˆØ¯) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ…ØŒ Ø¢ÛŒØ§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Q Ø¨Ù‡ÛŒÙ†Ù‡ Ù‡Ù…Ú¯Ø±Ø§ Ø´ÙˆØ¯ØŸ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… SARSA Ú†Ø·ÙˆØ±ØŸ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design $r(s)$ and $\\gamma$\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "    Ø¯Ø± <code>GAMMA = 0.9</code> Ù…ØªØºÛŒØ± <code>rewards</code> Ø¨Ø§Ø²ÛŒ Ø±Ø§ Ø·ÙˆØ±ÛŒ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒØ¯ Ú©Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø¹Ù…Ù„Ú©Ø±Ø¯ ÛŒÚ© Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø±Ø§ Ø§Ø² Ø¯Ùˆ Ù†Ø¸Ø± Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br />\n",
    "Û±) Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¬ÙˆØ§Ø¨ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ú¯Ø±Ø¯Ø¯.    \n",
    "<br />    \n",
    " Û²) Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù† training Ø¹Ù…Ù„Ú©Ø±Ø¯ Agent Ø§Ø² Ù†Ø¸Ø± Ø§Ø­ØªÙ…Ø§Ù„ Ø±Ø³ÛŒØ¯Ù† Ø¢Ù† Ø¨Ù‡ Ú¯Ù†Ø¬ Ø¨Ù‡ØªØ± Ø¨Ø§Ø´Ø¯.\n",
    "Ø§ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Policy Iteration Ø§Ø² Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ <code>policy_track</code> Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒØŒ Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Value Iteration Ø§Ø² Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ <code>value_track</code> Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ùˆ Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Q-Learning Ø§Ø² <code>final_state_track</code> Ø¯Ø± Ø®Ø±ÙˆØ­ÛŒ Ù‚Ø§Ø¨Ù„ Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø³Øª.\n",
    "<br />   \n",
    "    Ø­Ø§Ù„ Ú†Ù†Ø¯ Ø³Ù†Ø§Ø±ÛŒÙˆ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ø§ÛŒ <code>rewards</code> Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¨Ù‡ Ø§Ø²Ø§ÛŒ <code>GAMMA</code> Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø§Ø² Ø¬Ù…Ù„Ù‡ <code>GAMMA = 1</code> Ù…Ù‚Ø§ÛŒØ³Ù‡ Ùˆ Ø¢Ù†Ø§Ù„ÛŒØ² Ú©Ù†ÛŒØ¯.\n",
    "</font> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Design a fun map!\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "    Ø¨Ù‡ Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø®ÙˆØ¯ ÛŒÚ© Ù†Ù‚Ø´Ù‡â€ŒÛŒ Ú¯Ù†Ø¬ Ø¬Ø§Ù„Ø¨ Ø·Ø±Ø§Ø­ÛŒ Ú©Ù†ÛŒØ¯ Ùˆ Ø±Ø¨Ø§Øª Ø±Ø§ Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Q-Learning Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯. Ø¨Ø§ <code>gif</code> Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø¨Ø§Øª Ø±Ø§ Ù†Ø´Ø§Ù† Ø¯Ù‡ÛŒØ¯.\n",
    " </font> </div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give Us Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "145a46d259b94d49be1408277befd7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Label(value='How was the homework?'),)), HBox(children=(Label(value='Please rateâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Label, IntSlider, Textarea, Button, Layout, HBox, VBox\n",
    "from IPython.display import clear_output\n",
    "\n",
    "IntSlider_list = {i:IntSlider(value=3, min=1, max=5) for i in range(6)}\n",
    "H1 = HBox([Label(\"How was the homework?\")])\n",
    "H2 = HBox([Label(\"Please rate the homework from 1 to 5 (lowest to highest).\")])\n",
    "H3 = HBox([Label(\"The assignment was instructive.\", layout=Layout(width=\"22%\")), IntSlider_list[0]])\n",
    "H4 = HBox([Label(\"It was not time consuming.\", layout=Layout(width=\"22%\")), IntSlider_list[1]])\n",
    "H5 = HBox([Label(\"The questions were clear.\", layout=Layout(width=\"22%\")), IntSlider_list[2]])\n",
    "H6 = HBox([Label(\"The environment was interesting.\", layout=Layout(width=\"22%\")), IntSlider_list[3]])\n",
    "H7 = HBox([Label(\"The homework was well organized.\", layout=Layout(width=\"22%\")), IntSlider_list[4]])\n",
    "H8 = HBox([Label(\"Finally, you had a good vibe!\", layout=Layout(width=\"22%\")), IntSlider_list[5]])\n",
    "textarea = Textarea(value='', placeholder='Any comments or suggestions', description='', disabled=False)\n",
    "button = Button(description='Submit', disabled=False, button_style='', tooltip='Click me', icon='paper-plane')\n",
    "H9 = HBox([textarea])\n",
    "H10 = HBox([button])\n",
    "\n",
    "def ff(button):\n",
    "    button.button_style = 'success'\n",
    "    score = sum([IntSlider_list[i].value for i in IntSlider_list]) / 6\n",
    "    x = round(score*4)/4\n",
    "    y = int(x) * 'ğŸŒ•' + ((x-int(x))==0 and not x==5) * 'ğŸŒ‘' + ((x-int(x))==0.25) * 'ğŸŒ˜' + ((x-int(x))==0.5) * 'ğŸŒ—' + ((x-int(x))==0.75) * 'ğŸŒ–' + (4-int(x)) * 'ğŸŒ‘'\n",
    "    clear_output(wait=True)\n",
    "    display(VBox([H1, H2, H3, H4, H5, H6, H7, H8, H9, H10]))\n",
    "    print(\"Thank you for your feedback! ğŸ˜Š\")\n",
    "    print(f\"Score: {y}\")\n",
    "    print(\"\\nFeedback:\",*[IntSlider_list[i].value for i in IntSlider_list])\n",
    "    print(textarea.value)\n",
    "\n",
    "button.on_click(ff)\n",
    "\n",
    "VBox([H1, H2, H3, H4, H5, H6, H7, H8, H9, H10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To export jupyter notebook to html, save your notebook and run this cell\n",
    "!jupyter nbconvert --to html CHW1.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
