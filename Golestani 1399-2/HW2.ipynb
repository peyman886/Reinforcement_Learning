{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" class=\"alert alert-block alert-info\"> <font size=\"5\" face=\"HM XNiloofar\"> \n",
    "<b> ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªÙ‚ÙˆÛŒØªÛŒ - Ø¯Ú©ØªØ± Ú¯Ù„Ø³ØªØ§Ù†ÛŒ: ØªÙ…Ø±ÛŒÙ† Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÙˆÙ… </b>\n",
    "</font> </div>\n",
    "\n",
    "<div dir=\"rtl\"> <font size=\"4\" face=\"HM XNiloofar\"> \n",
    "<br />\n",
    "Ù…Ù‡Ù„Øª ØªØ­ÙˆÛŒÙ„ ØªÙ…Ø±ÛŒÙ†: Û³Û° Ø®Ø±Ø¯Ø§Ø¯ Û±Û´Û°Û± <br />\n",
    "ÙÙ‚Ø· Ù‚Ø³Ù…Øªâ€ŒÙ‡Ø§ÛŒ <code>TODO</code> Ø±Ø§ Ù¾Ø± Ú©Ù†ÛŒØ¯ Ùˆ  Jupyter Notebook ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª <code>ipynb</code> Ùˆ <code>html</code> Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø¯Ø± CW Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯. <br />\n",
    "Ø§Ø¨Ù‡Ø§Ù…Ø§Øª Ùˆ Ø³ÙˆØ§Ù„Ø§Øª Ø®ÙˆØ¯ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø±Ø§ Ù…ÛŒ ØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ø§ Ø·Ø±Ø§Ø­ ØªÙ…Ø±ÛŒÙ† Ù…Ø·Ø±Ø­ Ú©Ù†ÛŒØ¯. <br />\n",
    " <div dir=\"ltr\">@hamidreza_ehteram</div> <br />  \n",
    "Ø¯Ø± Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¯Ø± ÛŒÚ© Ù…Ø­ÛŒØ· Ø¨Ø§Ø²ÛŒ Multi-armed bandit Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ optimistic-initial ØŒzero-initial ØŒtau-softmax ØŒepsilon-greedy incremental implementation Ùˆ gradient bandit algorithms ØŒ Ø±Ø§ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ Ùˆ Ø¨Ø§ ØªØ§Ø«ÛŒØ± Ù¾Ø§Ø±Ø§Ù…ØªØ±â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¯Ø± Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ Ø¢Ø´Ù†Ø§ Ù…ÛŒâ€ŒØ´ÙˆÛŒØ¯. <br />\n",
    "</font> </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***You can jump directly to these sections:***\n",
    "\n",
    "+ [A. Preparations](#A.-Preparations)\n",
    "    + [1. Reward function](#1.-Reward-function)\n",
    "    + [2. epsilon-greedy policy](#2.-epsilon-greedy-policy)\n",
    "    + [3. tau-softmax policy](#3.-tau-softmax-policy)\n",
    "+ [B. Learning](#B.-Learning)\n",
    "    + [1. Zero-initial and epsilon-greedy policy](#1.-Zero-initial-and-epsilon-greedy-policy)\n",
    "    + [2. Optimistic-initial and epsilon-greedy policy](#2.-Optimistic-initial-and-epsilon-greedy-policy)\n",
    "    + [3. Zero-initial and tau-softmax policy](#3.-Zero-initial-and-tau-softmax-policy)\n",
    "    + [4. Optimistic-initial and tau-softmax policy](#4.-Optimistic-initial-and-tau-softmax-policy)\n",
    "    + [5. Gradient Bandit Algorithms](#5.-Gradient-Bandit-Algorithms)\n",
    "    + [6. Incremental implementation](#6.-Incremental-implementation)\n",
    "    + [7. High-variance environment](#7.-High-variance-environment)\n",
    "+ [Give Us Feedback](#Give-Us-Feedback)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The multi-armed bandit model ([Wikipedia])**\n",
    "\n",
    "The multi-armed bandit (short: bandit or MAB) can be seen as a set of real distributions $B=\\{R_{1},\\dots ,R_{K}\\}$, each distribution being associated with the rewards delivered by one of the $K\\in \\mathbb {N} ^{+}$ levers. Let $\\mu _{1},\\dots ,\\mu _{K}$ be the mean values associated with these reward distributions. The gambler iteratively plays one lever per round and observes the associated reward. The objective is to maximize the sum of the collected rewards. The horizon $H$ is the number of rounds that remain to be played. The bandit problem is formally equivalent to a one-state Markov decision process. The regret $\\rho$  after $T$ rounds is defined as the expected difference between the reward sum associated with an optimal strategy and the sum of the collected rewards:\n",
    "\n",
    "$\\rho =T\\mu ^{*}-\\sum _{t=1}^{T}{\\widehat {r}}_{t},$\n",
    "\n",
    "where $\\mu ^{*}$ is the maximal reward mean, $\\mu ^{*}=\\max _{k}\\{\\mu _{k}\\}$, and ${\\widehat {r}}_{t}$ is the reward in round $t$.\n",
    "\n",
    "A zero-regret strategy is a strategy whose average regret per round $\\rho /T$ tends to zero with probability 1 when the number of played rounds tends to infinity. Intuitively, zero-regret strategies are guaranteed to converge to a (not necessarily unique) optimal strategy if enough rounds are played.\n",
    "\n",
    "[Wikipedia]:https://en.wikipedia.org/wiki/Multi-armed_bandit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin: 20px\" class=\"alert alert-block alert-success\"> <font size=\"4\"> <b> Game environment:</b> We have 11 arms ğŸ° with associated rewards $[R_1, \\ldots, R_{11}]$ where $$ R_k \\sim \\mathcal{N}(k-6, 1).$$ </font> </div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T10:05:49.318620Z",
     "start_time": "2019-11-04T10:05:49.314619Z"
    },
    "id": "OfiVTUtPC45H"
   },
   "source": [
    "# A. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T18:55:56.721412Z",
     "start_time": "2019-11-04T18:55:52.412658Z"
    },
    "id": "8h8xp0V_C44-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to see the reward functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T14:16:02.779015Z",
     "start_time": "2019-11-04T14:16:02.439585Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "DQpJiaf4C45I",
    "outputId": "d06dfe55-6f55-4be2-fb4c-7bece8d5bd34"
   },
   "outputs": [],
   "source": [
    "# Reward Distributions\n",
    "mus = np.array([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])\n",
    "sigmas = np.array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "num_arms = len(mus)\n",
    "sample_size = 10000\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "for mu, sigma, idx in zip(mus, sigmas, range(num_arms)):\n",
    "    sample = np.random.normal(mu, sigma, sample_size)\n",
    "    plt.hist(sample, bins=50, density=True, alpha=0.8, label=f'arm {idx+1}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Svf76cZC45J"
   },
   "source": [
    "## 1. Reward function\n",
    "\n",
    "Define reward function which gets arm number as input and returns reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T15:17:19.804029Z",
     "start_time": "2019-11-04T15:17:19.798024Z"
    },
    "id": "eHqCspsyC45J"
   },
   "outputs": [],
   "source": [
    "def reward(arm):\n",
    "    # TODO\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yC6nUV2AC45K",
    "outputId": "36739fe4-6416-4abb-f8dc-59baa21b6f31"
   },
   "outputs": [],
   "source": [
    "reward(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkEPjR-XC45K"
   },
   "source": [
    "**Optimistic initial action values** \n",
    "\n",
    "See Reinforcement Learning: An Introduction, Second edition, By Richard S. Sutton, Andrew G. Barto, Section 2.6;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvS4iztMC45L"
   },
   "outputs": [],
   "source": [
    "Qs = 10 + 0.01 * np.random.rand(num_arms)\n",
    "Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgNrkQwhC45L"
   },
   "source": [
    "## 2. epsilon-greedy policy\n",
    "\n",
    "Define decision function for action selection using $\\epsilon$-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0Szvp7-C45M"
   },
   "outputs": [],
   "source": [
    "def select_arm_e_greedy(Qs, epsilon):\n",
    "    # TODO\n",
    "    return arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYgDOvgzC45M",
    "outputId": "6ef43503-bd0a-4f93-fed2-b72d73cc96a0"
   },
   "outputs": [],
   "source": [
    "select_arm_e_greedy(Qs,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ti9N0-PrWVbT"
   },
   "source": [
    "## 3. tau-softmax policy\n",
    "\n",
    "**Softmax Action Selection**\n",
    "\n",
    "[Reinforcement Learning: An Introduction], First edition, By Richard S. Sutton, Andrew G. Barto, Section 2.3;\n",
    "\n",
    "\n",
    "\n",
    "Although $\\varepsilon $-greedy action selection is an effective and popular means of balancing exploration and exploitation in reinforcement learning, one drawback is that when it explores it chooses equally among all actions. This means that it is as likely to choose the worst-appearing action as it is to choose the next-to-best action. In tasks where the worst actions are very bad, this may be unsatisfactory. The obvious solution is to vary the action probabilities as a graded function of estimated value. The greedy action is still given the highest selection probability, but all the others are ranked and weighted according to their value estimates. These are called softmax action selection rules. The most common softmax method uses a Gibbs, or Boltzmann, distribution. It chooses action $a$ on the $t-$th play with probability:\n",
    "\n",
    "$$\\frac{ e^{Q(a)/ \\tau} }{\\sum_a e^{Q(a)/ \\tau}}$$\n",
    "\n",
    "temprature ($\\tau$) in the above formula influences how randomly actions should be chosen:\n",
    "\n",
    "\n",
    "*   if $\\tau$ is high, the exponentials approach 1, the fraction approaches 1/(number of actions), and each action has approximately the same probablity of being chosen (exploration or exploitation?)\n",
    "*   as $\\tau \\rightarrow 0$, the exponential with the highest Q(a) dominates, and the current action is always chosen (exploration or exploitation?)\n",
    "\n",
    "\n",
    "[Reinforcement Learning: An Introduction]:https://books.google.nl/books?id=U57uDwAAQBAJ&printsec=frontcover#v=onepage&q&f=false\n",
    "\n",
    "\n",
    "\n",
    "Define decision function for action selection using softmax policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90hUX6a7C45N"
   },
   "outputs": [],
   "source": [
    "def select_arm_softmax(Qs, temperature):\n",
    "    # TODO\n",
    "    return arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y9yfa03QC45N",
    "outputId": "00064835-727e-4e90-fc83-5eceab1ae4e7"
   },
   "outputs": [],
   "source": [
    "select_arm_softmax(Qs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJcR-MiqC45O"
   },
   "source": [
    "# B. Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zero-initial and epsilon-greedy policy\n",
    "\n",
    "With zero-initial `Qs = 0 + 0.01 * np.random.rand(num_arms)`, train an agent for 500 trials using $\\varepsilon=$ `[0.0, 0.01, 0.1, 0.5, 'dec']`-greedy policy. Repeat it 1000 times and then average. Plot average rewards against trials (all plots should be in a single figure). Analyze the results (in terms of convergence, average reward over the 500 trials, etc.). `'dec'` indicates that $\\varepsilon$ decreases linearly from `0.2` to `0.0` during the training. [Here](https://drive.google.com/file/d/1RM8W-9NmcZIP7opNrk5F9hrFWHIaByua/view?usp=sharing) is what we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggestion:** To compare the performance of different scenarios, in a table or figure, compare *(I) the average reward at the end of the training (500th trial)*, *(II) the average reward during the training (over 500 trials)*, and *(III) any other desired metrics* for each scenario (see Figure 2.6 in Reinforcement Learning: An Introduction, Second edition, By Richard S. Sutton, Andrew G. Barto, Section 2.10;)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimistic-initial and epsilon-greedy policy\n",
    "\n",
    "With optimistic-initial `Qs = 10 + 0.01 * np.random.rand(num_arms)`, train an agent for 500 trials using $\\varepsilon=$ `[0.0, 0.01, 0.1, 0.5, 'dec']`-greedy policy. Repeat it 1000 times and then average. Plot average rewards against trials. Compare the result with [Question 1](#1.-Zero-initial-and-epsilon-greedy-policy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-initial and tau-softmax policy\n",
    "\n",
    "With zero-initial `Qs = 0 + 0.01 * np.random.rand(num_arms)`, train an agent for 500 trials using $\\tau=$ `[0.1, 1.0, 2.0, 10.0, 'dec']`-softmax policy. Repeat it 1000 times and then average. Plot average rewards against trials. `'dec'` indicates that $\\tau$ decreases linearly from `1.0` to `0.1` during the training. Compare the result with [Question 1](#1.-Zero-initial-and-epsilon-greedy-policy).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimistic-initial and tau-softmax policy\n",
    "\n",
    "With optimistic-initial `Qs = 10 + 0.01 * np.random.rand(num_arms)`, train an agent for 500 trials using $\\tau=$ `[0.1, 1.0, 2.0, 10.0, 'dec']`-softmax policy. Repeat it 1000 times and then average. Plot average rewards against trials. Compare the result with the previous questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Gradient Bandit Algorithms\n",
    "\n",
    "Reinforcement Learning: An Introduction, Second edition, By Richard S. Sutton, Andrew G. Barto, Section 2.8;\n",
    "\n",
    "$$Pr\\{A_t = a\\} = \\frac{e^{H_t(a)}}{\\sum_{b=1}^k e^{H_t(b)}} = \\pi_t(a)$$\n",
    "\n",
    "On each step, after selecting action $A_t$ and receiving the reward $R_t$, the action preferences are updated by:\n",
    "$$H_{t+1}(a) = H_t(a) + \\alpha (R_t - \\bar{R}_t) \\big(1_{a=A_t} - \\pi_t(a)\\big), \\forall a,$$\n",
    "where $\\bar{R}_t$ is the average of all the rewards, i.e.,\n",
    "$$\\bar{R}_t = \\frac{1}{t}(R_1 + \\ldots + R_t).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With zero-initial `Hs = 0 + 0.01 * np.random.rand(num_arms)`, train an agent for 500 trials using $\\alpha=$ `[0.01, 0.1, 0.5, 1.0, 'dec']`. Repeat it 1000 times and then average. Plot average rewards against trials. Analyze the results. `'dec'` indicates that $\\alpha$ decreases linearly from `0.15` to `0.05` during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Incremental implementation\n",
    "\n",
    "With zero-initial `Qs = 0 + 0.01 * np.random.rand(num_arms)`, train an agent for 500 trials using $\\varepsilon =$ `0.1`-greedy policy and $\\alpha=$ `[0.01, 0.1, 0.5, 1.0, 'dec']`. Repeat it 1000 times and then average. Plot average rewards against trials. Analyze the results. `'dec'` indicates that $\\alpha = 1/N$, that is, it calculates the exact average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. High-variance environment\n",
    "\n",
    "Let the variance of all arms be 10. Repeat all previous questions. Compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer: ```TODO ```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give Us Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Label, IntSlider, Textarea, Button, Layout, HBox, VBox\n",
    "from IPython.display import clear_output\n",
    "\n",
    "IntSlider_list = {i:IntSlider(value=3, min=1, max=5) for i in range(6)}\n",
    "H1 = HBox([Label(\"How was the homework?\")])\n",
    "H2 = HBox([Label(\"Please rate the homework from 1 to 5 (lowest to highest).\")])\n",
    "H3 = HBox([Label(\"The assignment was instructive.\", layout=Layout(width=\"22%\")), IntSlider_list[0]])\n",
    "H4 = HBox([Label(\"It was not time consuming.\", layout=Layout(width=\"22%\")), IntSlider_list[1]])\n",
    "H5 = HBox([Label(\"The questions were clear.\", layout=Layout(width=\"22%\")), IntSlider_list[2]])\n",
    "H6 = HBox([Label(\"The environment was interesting.\", layout=Layout(width=\"22%\")), IntSlider_list[3]])\n",
    "H7 = HBox([Label(\"The homework was well organized.\", layout=Layout(width=\"22%\")), IntSlider_list[4]])\n",
    "H8 = HBox([Label(\"Finally, you had a good vibe!\", layout=Layout(width=\"22%\")), IntSlider_list[5]])\n",
    "textarea = Textarea(value='', placeholder='Any comments or suggestions', description='', disabled=False)\n",
    "button = Button(description='Submit', disabled=False, button_style='', tooltip='Click me', icon='paper-plane')\n",
    "H9 = HBox([textarea])\n",
    "H10 = HBox([button])\n",
    "\n",
    "def ff(button):\n",
    "    button.button_style = 'success'\n",
    "    score = sum([IntSlider_list[i].value for i in IntSlider_list]) / 6\n",
    "    x = round(score*4)/4\n",
    "    y = int(x) * 'ğŸŒ•' + ((x-int(x))==0 and not x==5) * 'ğŸŒ‘' + ((x-int(x))==0.25) * 'ğŸŒ˜' + ((x-int(x))==0.5) * 'ğŸŒ—' + ((x-int(x))==0.75) * 'ğŸŒ–' + (4-int(x)) * 'ğŸŒ‘'\n",
    "    clear_output(wait=True)\n",
    "    display(VBox([H1, H2, H3, H4, H5, H6, H7, H8, H9, H10]))\n",
    "    print(\"Thank you for your feedback! ğŸ˜Š\")\n",
    "    print(f\"Score: {y}\")\n",
    "    print(\"\\nFeedback:\",*[IntSlider_list[i].value for i in IntSlider_list])\n",
    "    print(textarea.value)\n",
    "\n",
    "button.on_click(ff)\n",
    "\n",
    "VBox([H1, H2, H3, H4, H5, H6, H7, H8, H9, H10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To export jupyter notebook to html, save your notebook and run this cell\n",
    "!jupyter nbconvert --to html CHW2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "epsilon_greedy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
